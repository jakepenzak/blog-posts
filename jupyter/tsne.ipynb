{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "import os\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.chdir(\"assets/articles/notebooks\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# t-SNE from Scratch (ft. NumPy):\n",
    "<center> **Acquire a deep understanding of the inner workings of t-SNE via implementation from scratch in python** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "I have found that one of the best ways to truly understanding any statistical algorithm or methodology is to manually implement it yourself. On the flip side, coding these algorithms can sometimes be time consuming and a real pain, and when somebody else has already done it, why would I want to spend my time doing it ‚Äî seems inefficient, no? Both are fair points, and I am not here to make an argument for one over the other.\n",
    "\n",
    "This article is designed for readers who are interested in understanding t-SNE via translation of the mathematics in the [original paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) ‚Äî by Laurens van der Maaten & Geoffrey Hinton ‚Äî into python code implementation.[1] I find these sort of exercises to be quite illuminating into the inner workings of statistical algorithms/models and really test your underlying understanding and assumptions regarding these algorithms/models. You are almost guaranteed to walk away with a better understanding then you had before. At the very minimum, successful implementation is always very satisfying!\n",
    "\n",
    "This article will be accessible to individuals with any level of exposure of t-SNE. However, note a few things this post definitely is **not**:\n",
    "\n",
    "1. A _strictly_ conceptual introduction and exploration of t-SNE, as there are plenty of other great resources out there that do this; nevertheless, I will be doing my best to connect the mathematical equations to their intuitive/conceptual counterparts at each stage of implementation.\n",
    "\n",
    "2. A _comprehensive_ discussion into the applications & pros/cons of t-SNE, as well as direct comparisons of t-SNE to other dimensionality reduction techniques. I will, however, briefly touch on these topics throughout this article, but will by no means cover this in-depth.\n",
    "\n",
    "Without further ado, let‚Äôs start with a _brief_ introduction into t-SNE.\n",
    "\n",
    "## A Brief Introduction to t-SNE\n",
    "\n",
    "_t-distributed stochastic neighbor embedding_ (t-SNE) is a dimensionality reduction tool that is primarily used in datasets with a large dimensional feature space and enables one to visualize the data down, or project it, into a lower dimensional space (usually 2-D). It is especially useful for visualizing non-linearly separable data wherein linear methods such as [Principal Component Analysis](https://en.m.wikipedia.org/wiki/Principal_component_analysis) (PCA) would fail. Generalizing linear frameworks of dimensionality reduction (such as PCA) into non-linear approaches (such as t-SNE) is also known as [Manifold Learning](https://en.m.wikipedia.org/wiki/Nonlinear_dimensionality_reduction). These methods can be extremely useful for visualizing and understanding the underlying structure of a high dimensional, non-linear data set, and can be great for disentangling and grouping together observations that are similar in the high-dimensional space. For more information on t-SNE and other manifold learning techniques, the [scikit-learn documentation](https://scikit-learn.org/stable/modules/manifold.html) is a great resource. Additionally, to read about some cool areas t-SNE has seen applications, the [Wikipedia page](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#cite_note-3) highlights some of these areas with references to the work.\n",
    "\n",
    "Let‚Äôs start with breaking down the name _t-distributed stochastic neighbor embedding_ into its components. t-SNE is an extension on stochastic neighbor embedding (SNE) presented 6 years earlier in this [paper](https://cs.nyu.edu/~roweis/papers/sne_final.pdf) by Geoffrey Hinton & Sam Roweis. So let‚Äôs start there. The _stochastic_ part of the name comes from the fact that the objective function is not convex and thus different results can arise from different initializations. The _neighbor embedding_ highlights the nature of the algorithm ‚Äî optimally mapping the points in the original high-dimensional space into the corresponding low-dimensional space while best preserving the ‚Äúneighborhood‚Äù structure of the points. SNE is comprised of the following (simplified) steps:\n",
    "\n",
    "1. _Obtain the Similarity Matrix between Points in the Original Space_: Compute the conditional probabilities for each datapoint $j$ relative to each datapoint $i$. These conditional probabilities are calculated in the original high-dimensional space using a Gaussian centered at $i$ and take on the following interpretation: the probability that i would pick $j$ as its neighbor in the original space. This creates a matrix that represents similarities between the points.\n",
    "\n",
    "2. _Initialization_: Choose random starting points in the lower-dimensional space (say, 2-D) for each datapoint in the original space and compute new conditional probabilities similarly as above in this new space.\n",
    "\n",
    "3. _Mapping_: Iteratively improve upon the points in the lower-dimensional space until the [Kullback-Leibler divergences](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between all the conditional probabilities is minimized. Essentially we are minimizing the differences in the probabilities between the similarity matrices of the two spaces so as to ensure the similarities are best preserved in the mapping of the original high-dimensional dataset to the low-dimensional dataset.\n",
    "\n",
    "t-SNE improves upon SNE in two primary ways:\n",
    "\n",
    "1. It minimizes the Kullback-Leibler divergences between the joint probabilities rather than the conditional probabilities. The authors refer to this as ‚Äúsymmetric SNE‚Äù b/c their approach ensures that the joint probabilities $p_ij$ = $p_ji$. **This results in a much better behaved cost function that is easier to optimize.**\n",
    "\n",
    "2. It computes the similarities between points using a [student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) w/ one degree of freedom (also a [Cauchy Distribution](https://en.wikipedia.org/wiki/Cauchy_distribution)) rather than a Gaussian in the low-dimensional space (step 2 above). Here we can see where the ‚Äút‚Äù in t-SNE is coming from. **This improvement helps to alleviate the ‚Äúcrowding problem‚Äù highlighted by the authors and to further improve the optimization problem.** This ‚Äúcrowding problem‚Äù can be envisioned as such: Imagine we have a 10-D space, the amount of space available in 2-D will not be sufficient to accurately capture those moderately dissimilar points compared to the amount of space for nearby points relative to the amount of space available in a 10-D space. More simply, just envision taking a 3-D space and projecting it down to 2-D, the 3-D space will have much more overall space to model the similarities relative to the projection down into 2-D. The Student-t distribution helps alleviate this problem by having heavier tails than the normal distribution. See the [original paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) for a much more in-depth treatment of this problem.\n",
    "\n",
    "If this did not all track immediately, that is okay! I am hoping when we implement this in code, the pieces will all fall in to place. The main takeaway is this: **t-SNE models similarities between datapoints in the high-dimensional space using joint probabilities of ‚Äúdatapoints choosing others as its neighbor‚Äù, and then tries to find the best mapping of these points down into the low-dimensional space, while best preserving the original high-dimensional similarities.**\n",
    "\n",
    "## Implementation from Scratch\n",
    "\n",
    "Let‚Äôs now move on to understanding t-SNE via implementing the original version of the algorithm as presented in the [paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) by Laurens van der Maaten & Geoffrey Hinton. We will first start with implementing algorithm 1 below step-by-step, which will cover 95% of the main algorithm. There are two additional enhancements the authors note: 1) Early Exaggeration & 2) Adaptive Learning Rates. We will only discuss adding in the early exaggeration as that is most conducive in aiding the interpretation of the actual algorithms inner workings, as the adaptive learning rate is focused on improving the rates of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.image(\"data/algorithm.webp\").center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "### 1. Inputs\n",
    "\n",
    "Following the original paper, we will be using the publicly available [MNIST dataset](https://www.openml.org/search?type=data&status=active&id=554&sort=runs) from OpenML with images of handwritten digits from 0‚Äì9.[2] We will also randomly sample 1000 images from the dataset & reduce the dimensionality of the dataset using Principal Component Analysis (PCA) and keep 30 components. These are both to improve computational time of the algorithm, as the code here is not optimized for speed, but rather for interpretability & learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch MNIST data\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X_total = pd.DataFrame(mnist[\"data\"])\n",
    "y_total = pd.DataFrame(mnist[\"target\"])\n",
    "\n",
    "X_reduced = X_total.sample(n=1000)\n",
    "y_reduced = y_total.loc[X_reduced.index]\n",
    "\n",
    "# PCA to keep 30 components\n",
    "X = PCA(n_components=30).fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {},
   "source": [
    "This will be our X dataset with each row being an image and each column being a feature, or principal component in this case (i.e. linear combinations of the original pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# To make it look pretty in marimo\n",
    "df_X = pd.DataFrame(X)\n",
    "df_X.columns = df_X.columns.astype(str)\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "We also will need to specify the cost function parameters ‚Äî perplexity ‚Äî and the optimization parameters ‚Äî iterations, learning rate, & momentum. We will hold off on these for now and address them as they appear at each stage.\n",
    "\n",
    "In terms of output, recall that we seek a the low-dimensional mapping of the original dataset X. We will be mapping the original space into a 2 dimensional space throughout this example. Thus, our new output will be the 1000 images now represented in a 2 dimensional space rather than the original 30 dimensional space, with a shape of [1000, 2].\n",
    "\n",
    "### 2. Compute Affinities/Similarities of X in the Original Space\n",
    "\n",
    "Now that we have our inputs, the first step is to compute the pairwise similarities in the original high dimensional space. That is, for each image $i$ we compute the probability that $i$ would pick image $j$ as its neighbor in the original space for each $j$. These probabilities are calculated via a normal distribution centered around each point and then are normalized to sum up to 1. Mathematically, we have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_{j|i}=\\frac{\\exp{(-\\|x_i-x_j\\|^2/2\\sigma_i^2)}}{\\sum_{k\\ne i}\\exp{(-\\|x_i-x_j\\|^2/2\\sigma_i^2)}}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note that, in our case with n = 1000, these computations will result in a 1000 x 1000 matrix of similarity scores. Note we set $p$ = 0 whenever $i$ = $j$ b/c we are modeling pairwise similarities. However, you may notice that we have not mentioned how $\\sigma$ is determined. This value is determined for each observation $i$ via a grid search based on the user-specified desired [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the distributions. We will talk about this immediately below, but let‚Äôs first look at how we would code eq. (1) above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "Now before we look at the results of this code, let‚Äôs discuss how we determine the values of $\\sigma$ via the grid_search() function. Given a specified value of perplexity (which in this context can be loosely thought about as the number of nearest neighbors for each point), we do a grid search over a range of values of $\\sigma$ such that the following equation is as close to equality as possible for each $i$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Perp(P_i)=2^{H(P_i)}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $H(P_i)$ is the Shannon entropy of $P$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "H(P_i) = - \\sum_jp_{j|i}\\log_2p_{j|i}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In our case, we will set perplexity = 10 and set the search space to be defined by [0.01 * standard deviation of the norms for the difference between images $i$ and $j$, 5 * standard deviation of the norms for the difference between images $i$ and $j$] divided into 200 equal steps. Knowing this, we can define our grid_search() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(diff_i: np.ndarray, i: int, perplexity: int) -> float:\n",
    "    \"\"\"\n",
    "    Helper function to obtain œÉ's based on user-specified perplexity.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    diff_i\n",
    "        Array containing the pairwise differences between data points.\n",
    "    i\n",
    "        Index of the current data point.\n",
    "    perplexity\n",
    "        User-specified perplexity value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The value of œÉ that satisfies the perplexity condition.\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.inf  # Set first result to be infinity\n",
    "\n",
    "    norm = np.linalg.norm(diff_i, axis=1)\n",
    "    std_norm = np.std(norm)  # Use standard deviation of norms to define search space\n",
    "\n",
    "    for œÉ_search in np.linspace(0.01 * std_norm, 5 * std_norm, 200):\n",
    "        # Equation 1 Numerator\n",
    "        p = np.exp(-(norm**2) / (2 * œÉ_search**2))\n",
    "\n",
    "        # Set p = 0 when i = j\n",
    "        p[i] = 0\n",
    "\n",
    "        # Equation 1 (Œµ -> 0)\n",
    "        Œµ = np.nextafter(0, 1)\n",
    "        p_new = np.maximum(p / np.sum(p), Œµ)\n",
    "\n",
    "        # Shannon Entropy\n",
    "        H = -np.sum(p_new * np.log2(p_new))\n",
    "\n",
    "        # Get log(perplexity equation) as close to equality\n",
    "        if np.abs(np.log(perplexity) - H * np.log(2)) < np.abs(result):\n",
    "            result = np.log(perplexity) - H * np.log(2)\n",
    "            œÉ = œÉ_search\n",
    "\n",
    "    return œÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_pairwise_affinities(X: np.ndarray, perplexity: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to obtain affinities matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        The input data array.\n",
    "    perplexity\n",
    "        The perplexity value for the grid search.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    np.ndarray\n",
    "        The pairwise affinities matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(X)\n",
    "\n",
    "    print(\"Computing Pairwise Affinities....\")\n",
    "\n",
    "    p_ij = np.zeros(shape=(n, n))\n",
    "    for i in range(0, n):\n",
    "        # Equation 1 numerator\n",
    "        diff = X[i] - X\n",
    "        œÉ_i = grid_search(diff, i, perplexity)  # Grid Search for œÉ_i\n",
    "        norm = np.linalg.norm(diff, axis=1)\n",
    "        p_ij[i, :] = np.exp(-(norm**2) / (2 * œÉ_i**2))\n",
    "\n",
    "        # Set p = 0 when j = i\n",
    "        np.fill_diagonal(p_ij, 0)\n",
    "\n",
    "        # Equation 1\n",
    "        p_ij[i, :] = p_ij[i, :] / np.sum(p_ij[i, :])\n",
    "\n",
    "    # Set 0 values to minimum numpy value (Œµ approx. = 0)\n",
    "    Œµ = np.nextafter(0, 1)\n",
    "    p_ij = np.maximum(p_ij, Œµ)\n",
    "\n",
    "    print(\"Completed Pairwise Affinities Matrix. \\n\")\n",
    "\n",
    "    return p_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "Given these functions, we can compute the affinity matrix via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij = get_original_pairwise_affinities(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# To make it look pretty in marimo\n",
    "df_p_ij = pd.DataFrame(p_ij)\n",
    "df_p_ij.columns = df_p_ij.columns.astype(\"str\")\n",
    "df_p_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "Note, the diagonal elements are set to $\\epsilon \\approx 0$ by construction (whenever $i$ = $j$). Recall that a key extension of the t-SNE algorithm is to compute the joint probabilities rather than the conditional probabilities. This is computed simply as follow:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_{ij}=\\frac{p_{j|i}+p_{i|j}}{2n}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, we can define a new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symmetric_p_ij(p_ij: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to obtain symmetric affinities matrix utilized in t-SNE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_ij\n",
    "        The input affinity matrix.\n",
    "\n",
    "    Returns\n",
    "    np.ndarray\n",
    "        The symmetric affinities matrix.\n",
    "    \"\"\"\n",
    "    print(\"Computing Symmetric p_ij matrix....\")\n",
    "\n",
    "    n = len(p_ij)\n",
    "    p_ij_symmetric = np.zeros(shape=(n, n))\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            p_ij_symmetric[i, j] = (p_ij[i, j] + p_ij[j, i]) / (2 * n)\n",
    "\n",
    "    # Set 0 values to minimum numpy value (Œµ approx. = 0)\n",
    "    Œµ = np.nextafter(0, 1)\n",
    "    p_ij_symmetric = np.maximum(p_ij_symmetric, Œµ)\n",
    "\n",
    "    print(\"Completed Symmetric p_ij Matrix. \\n\")\n",
    "\n",
    "    return p_ij_symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "Feeding in `p_ij` from above, we obtain the following symmetric affinities matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij_symmetric = get_symmetric_p_ij(p_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# To make it look pretty in marimo\n",
    "df_p_ij_symmetric = pd.DataFrame(p_ij_symmetric)\n",
    "df_p_ij_symmetric.columns = df_p_ij_symmetric.columns.astype(\"str\")\n",
    "df_p_ij_symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {},
   "source": [
    "Now we have completed the first main step in t-SNE! We computed the symmetric affinity matrix in the original high-dimensional space. Before we dive right into the optimization stage, we will discuss the main components of the optimization problem in the next two steps and then combine them into our for loop.\n",
    "\n",
    "### 3. Sample Initial Solution & Compute Low Dimensional Affinity Matrix\n",
    "\n",
    "Now we want to sample a random initial solution in the lower dimensional space as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(\n",
    "    X: np.ndarray, n_dimensions: int = 2, initialization: str = \"random\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtain initial solution for t-SNE either randomly or using PCA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        The input data array.\n",
    "    n_dimensions\n",
    "        The number of dimensions for the output solution. Default is 2.\n",
    "    initialization\n",
    "        The initialization method. Can be 'random' or 'PCA'. Default is 'random'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The initial solution for t-SNE.\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError\n",
    "        If the initialization method is neither 'random' nor 'PCA'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample Initial Solution\n",
    "    if initialization == \"random\" or initialization != \"PCA\":\n",
    "        y0 = np.random.normal(loc=0, scale=1e-4, size=(len(X), n_dimensions))\n",
    "    elif initialization == \"PCA\":\n",
    "        X_centered = X - X.mean(axis=0)\n",
    "        _, _, Vt = np.linalg.svd(X_centered)\n",
    "        y0 = X_centered @ Vt.T[:, :n_dimensions]\n",
    "    else:\n",
    "        raise ValueError(\"Initialization must be 'random' or 'PCA'\")\n",
    "\n",
    "    return y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = initialization(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "df_y0 = pd.DataFrame(y0)\n",
    "df_y0.columns = df_y0.columns.astype(\"str\")\n",
    "df_y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {},
   "source": [
    "Now, we want to compute the affinity matrix in this lower dimensional space. However, recall that we do this utilizing a student's t-distribution w/ 1 degree of freedom:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "q_{ij}=\\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\sum_{k \\ne l} (1+\\|y_i-y_j\\|^2)^{-1}}\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Again, we set $q=0$ whenever $i = j$. Note this equation differs from eq. (1) in that the denominator is over all $i$ and thus symmetric by construction. Putting this into code, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_dimensional_affinities(Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtain low-dimensional affinities.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    Y\n",
    "        The low-dimensional representation of the data points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The low-dimensional affinities matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(Y)\n",
    "    q_ij = np.zeros(shape=(n, n))\n",
    "\n",
    "    for i in range(0, n):\n",
    "        # Equation 4 Numerator\n",
    "        diff = Y[i] - Y\n",
    "        norm = np.linalg.norm(diff, axis=1)\n",
    "        q_ij[i, :] = (1 + norm**2) ** (-1)\n",
    "\n",
    "    # Set p = 0 when j = i\n",
    "    np.fill_diagonal(q_ij, 0)\n",
    "\n",
    "    # Equation 4\n",
    "    q_ij = q_ij / q_ij.sum()\n",
    "\n",
    "    # Set 0 values to minimum numpy value (Œµ approx. = 0)\n",
    "    Œµ = np.nextafter(0, 1)\n",
    "    q_ij = np.maximum(q_ij, Œµ)\n",
    "\n",
    "    return q_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "Here we are seeking a 1000 x 1000 affinity matrix but now in the lower dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ij = get_low_dimensional_affinities(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# To make it look pretty in marimo\n",
    "df_q_ij = pd.DataFrame(q_ij)\n",
    "df_q_ij.columns = df_q_ij.columns.astype(\"str\")\n",
    "df_q_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {},
   "source": [
    "### 4. Compute Gradient of the Cost Function\n",
    "\n",
    "Recall, our cost function is the Kullback-Leibler divergence between joint probability distributions in the high dimensional space and low dimensional space:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C=\\text{KL}(P\\|Q)=\\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Intuitively, we want to minimize the difference in the affinity matrices $p_{ij}$ and $q_{ij}$ thereby best preserving the ‚Äúneighborhood‚Äù structure of the original space. The optimization problem is solved using gradient descent, but first let‚Äôs look at computing the gradient for the cost function above. The authors derive (see appendix A of the paper) the gradient of the cost function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j(p_{ij}-q_{ij})(1+\\|y_i-y_j\\|^2)^{-1}(y_i-y_j)\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In python, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(p_ij: np.ndarray, q_ij: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtain gradient of cost function at current point Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_ij\n",
    "        The joint probability distribution matrix.\n",
    "    q_ij\n",
    "        The Student's t-distribution matrix.\n",
    "    Y\n",
    "        The current point in the low-dimensional space.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    np.ndarray\n",
    "        The gradient of the cost function at the current point Y.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(p_ij)\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = np.zeros(shape=(n, Y.shape[1]))\n",
    "    for i in range(0, n):\n",
    "        # Equation 5\n",
    "        diff = Y[i] - Y\n",
    "        A = np.array([(p_ij[i, :] - q_ij[i, :])])\n",
    "        B = np.array([(1 + np.linalg.norm(diff, axis=1) ** 2) ** (-1)])\n",
    "        C = diff\n",
    "        gradient[i] = 4 * np.sum((A * B).T * C, axis=0)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "Feeding in the relevant arguments, we obtain the gradient as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = get_gradient(p_ij_symmetric, q_ij, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Make pretty in marimo\n",
    "df_gradient = pd.DataFrame(gradient)\n",
    "df_gradient.columns = df_gradient.columns.astype(\"str\")\n",
    "df_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {},
   "source": [
    "Now, we have all the pieces in order to solve the optimization problem!\n",
    "\n",
    "### 5. Iterate & Optimize the Low-Dimensional Mapping\n",
    "\n",
    "In order to update our low-dimensional mapping, we use [gradient descent with momentum](https://en.wikipedia.org/wiki/Gradient_descent) as outlined by the authors:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{Y}^{(t)}=\\mathcal{Y}^{(t-1)}+\\eta\\frac{\\partial C}{\\partial \\mathcal{Y}} + \\alpha(t)(\\mathcal{Y}^{(t-1)}-\\mathcal{Y}^{(t-2)})\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is our [learning rate](https://en.wikipedia.org/wiki/Learning_rate) and $\\alpha(t)$ is our momentum term as a function of time. The learning rate controls the step size at each iteration and the momentum term allows the optimization algorithm to gain inertia in the smooth direction of the search space, while not being bogged down by the noisy parts of the gradient. We will set $\\eta=200$ for our example and will fix $\\alpha(t)=0.5$ if $t < 250$ and $\\alpha(t)=0.8$ otherwise. We have all the components necessary above to compute to the update rule, thus we can now run our optimization over a set number of iterations $T$ (we will set $T=1000$).\n",
    "\n",
    "Before we set up for iteration scheme, let‚Äôs first introduce the enhancement the authors refer to as ‚Äúearly exaggeration‚Äù. This term is a constant that scales the original matrix of affinities $p_{ij}$. What this does is it places more emphasis on modeling the very similar points (high values in $p_{ij}$ from the original space) in the new space early on and thus forming ‚Äúclusters‚Äù of highly similar points. The early exaggeration is placed on at the beginning of the iteration scheme ($T<250$) and then turned off otherwise. Early exaggeration will be set to 4 in our case. We will see this in action in our visual below following implementation.\n",
    "\n",
    "Now, putting all of the pieces together for the algorithm, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(\n",
    "    X: np.ndarray,\n",
    "    perplexity: int = 10,\n",
    "    T: int = 1000,\n",
    "    Œ∑: int = 200,\n",
    "    early_exaggeration: int = 4,\n",
    "    n_dimensions: int = 2,\n",
    ") -> list[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    t-SNE (t-Distributed Stochastic Neighbor Embedding) algorithm implementation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        The input data matrix of shape (n_samples, n_features).\n",
    "    perplexity\n",
    "        The perplexity parameter. Default is 10.\n",
    "    T\n",
    "        The number of iterations for optimization. Default is 1000.\n",
    "    Œ∑\n",
    "        The learning rate for updating the low-dimensional embeddings. Default is 200.\n",
    "    early_exaggeration\n",
    "        The factor by which the pairwise affinities are exaggerated during the early iterations of optimization.\n",
    "        Default is 4.\n",
    "    n_dimensions\n",
    "        The number of dimensions of the low-dimensional embeddings.\n",
    "        Default is 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[np.ndarray, np.ndarray]\n",
    "        A list containing the final low-dimensional embeddings and the history of embeddings at each iteration.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "\n",
    "    # Get original affinities matrix\n",
    "    p_ij = get_original_pairwise_affinities(X, perplexity)\n",
    "    p_ij_symmetric = get_symmetric_p_ij(p_ij)\n",
    "\n",
    "    # Initialization\n",
    "    Y = np.zeros(shape=(T, n, n_dimensions))\n",
    "    Y_minus1 = np.zeros(shape=(n, n_dimensions))\n",
    "    Y[0] = Y_minus1\n",
    "    Y1 = initialization(X, n_dimensions)\n",
    "    Y[1] = np.array(Y1)\n",
    "\n",
    "    print(\"Optimizing Low Dimensional Embedding....\")\n",
    "    # Optimization\n",
    "    for t in range(1, T - 1):\n",
    "        # Momentum & Early Exaggeration\n",
    "        if t < 250:\n",
    "            Œ± = 0.5\n",
    "            early_exaggeration = early_exaggeration\n",
    "        else:\n",
    "            Œ± = 0.8\n",
    "            early_exaggeration = 1\n",
    "\n",
    "        # Get Low Dimensional Affinities\n",
    "        q_ij = get_low_dimensional_affinities(Y[t])\n",
    "\n",
    "        # Get Gradient of Cost Function\n",
    "        gradient = get_gradient(early_exaggeration * p_ij_symmetric, q_ij, Y[t])\n",
    "\n",
    "        # Update Rule\n",
    "        Y[t + 1] = Y[t] - Œ∑ * gradient + Œ± * (Y[t] - Y[t - 1])  # Use negative gradient\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if t % 50 == 0 or t == 1:\n",
    "            cost = np.sum(p_ij_symmetric * np.log(p_ij_symmetric / q_ij))\n",
    "            print(f\"Iteration {t}: Value of Cost Function is {cost}\")\n",
    "\n",
    "    print(\n",
    "        f\"Completed Low Dimensional Embedding: Final Value of Cost Function is {np.sum(p_ij_symmetric * np.log(p_ij_symmetric / q_ij))}\"\n",
    "    )\n",
    "    solution = Y[-1]\n",
    "\n",
    "    return solution, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {},
   "source": [
    "Now calling the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution, Y = tsne(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# To make pretty for marimo\n",
    "df_solution = pd.DataFrame(solution)\n",
    "df_solution.columns = df_solution.columns.astype(\"str\")\n",
    "df_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {},
   "source": [
    "where `solution` is the final 2-D mapping and `Y` is our mapped 2-D values at each step of the iteration. Plotting the evolution of `Y` where `Y[-1]` is our final 2-D mapping, we obtain (note how the algorithm behaves with early exaggeration on and off):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def tsne_evolution_fig():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"MNIST t-SNE\")\n",
    "    scat = ax.scatter(Y[1][:, 0], Y[1][:, 1], c=y_reduced, cmap=\"tab10\")\n",
    "    plt.colorbar(scat, ax=ax)\n",
    "\n",
    "    # t-SNE Descent Animation\n",
    "    ys = []\n",
    "    prelims = list(range(0, 50, 5))\n",
    "    early_range = list(range(50, 250, 10))\n",
    "    mid_range_1 = list(range(250, 300, 5))\n",
    "    mid_range_2 = list(range(300, 400, 10))\n",
    "    end_range = list(range(400, 1000, 50))\n",
    "\n",
    "    visual_range = (\n",
    "        prelims\n",
    "        + early_range\n",
    "        + mid_range_1\n",
    "        + mid_range_2\n",
    "        + end_range\n",
    "        + [999, 999, 999, 999, 999, 999, 999]\n",
    "    )\n",
    "\n",
    "    for i in visual_range:\n",
    "        ys.append(Y[i])\n",
    "\n",
    "    def strike(text):\n",
    "        result = \"\"\n",
    "        for c in text:\n",
    "            result = result + c + \"\\u0336\"\n",
    "        return result\n",
    "\n",
    "    def animate(iterations):\n",
    "        scat.set_offsets(ys[iterations])\n",
    "        if iterations < 31:\n",
    "            ax.text(\n",
    "                0.05,\n",
    "                1,\n",
    "                \"Early Exaggeration\",\n",
    "                horizontalalignment=\"center\",\n",
    "                verticalalignment=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "        else:\n",
    "            ax.text(\n",
    "                0.05,\n",
    "                1,\n",
    "                strike(\"                  \"),\n",
    "                horizontalalignment=\"center\",\n",
    "                verticalalignment=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(\n",
    "            [\n",
    "                1.25 * np.min(ys[iterations][:, 0]),\n",
    "                1.25 * np.max(ys[iterations][:, 0]),\n",
    "            ]\n",
    "        )\n",
    "        ax.set_ylim(\n",
    "            [\n",
    "                1.25 * np.min(ys[iterations][:, 1]),\n",
    "                1.25 * np.max(ys[iterations][:, 1]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    rot_animation = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(ys) - 1, interval=350, blit=False\n",
    "    )\n",
    "\n",
    "    rot_animation.save(\"data/MNIST.gif\", dpi=300)\n",
    "\n",
    "\n",
    "tsne_evolution_fig()\n",
    "mo.image(\"data/MNIST.gif\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SdmI",
   "metadata": {},
   "source": [
    "I recommend playing around with different values of the parameters (i.e., perplexity, learning rate, early exaggeration, etc.) to see how the solution differs (See the original paper and the scikit-learn documentation for guides on using these parameters).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it, we have coded t-SNE from scratch! I hope you have found this exercise to be illuminating into the inner workings of t-SNE and, at the very minimum, satisfying. Note that this implementation is not intended to be optimized for speed, but rather for understanding. Additions to the t-SNE algorithm have been implemented to improve computational speed and performance, such as variants of the [Barnes-Hut algorithm](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf) (tree-based approaches), using PCA as the initialization of the embedding, or using additional gradient descent extensions such as adaptive learning rates. The implementation in scikit-learn makes use of many of these enhancements.\n",
    "\n",
    "As always, I hope you have enjoyed reading this as much as I enjoyed writing it.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9:2579‚Äì2605, 2008.\n",
    "\n",
    "[2] LeCun et al. (1999): The MNIST Dataset Of Handwritten Digits (Images) License: CC BY-SA 3.0\n",
    "\n",
    "<div style=\"text-align: center; font-size: 24px;\">‚ùñ‚ùñ‚ùñ</div>\n",
    "\n",
    "<center>\n",
    "Access all the code via this Marimo Notebook or my [GitHub Repo](https://github.com/jakepenzak/blog-posts)\n",
    "\n",
    "I appreciate you reading my post! My posts primarily explore real-world and theoretical applications of econometric and statistical/machine learning techniques, but also whatever I am currently interested in or learning üòÅ. At the end of the day, I write to learn! I hope to make complex topics slightly more accessible to all.\n",
    "</center>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
