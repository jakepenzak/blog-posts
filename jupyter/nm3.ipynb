{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import marimo as mo\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import sympy as sm\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "import statsmodels.api as stats\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "try:\n",
    "    os.chdir(\"assets/articles/notebooks\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def display_iframe(path: str):\n",
    "    # Read the saved Plotly HTML file\n",
    "    with open(path, \"r\") as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Display it in Jupyter Notebook\n",
    "    return mo.iframe(html_content, height=\"500px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Optimization, Newton's Method, & Profit Maximization: Part 3 - Applied Profit Maximization\n",
    "<center> **Learn how to apply optimization & econometric techniques to solve an applied profit maximization problem** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> This article is the 3rd, and final, in a 3 part series. In the <a href=\"/articles/nm1\" target=\"_blank\" rel=\"noopener noreferrer\">1st part</a>, we studied basic optimization theory. Then, in <a href=\"/articles/nm1\" target=\"_blank\" rel=\"noopener noreferrer\">pt. 2</a>, we extended this theory to constrained optimization problems. Now, in pt. 3, we will apply the optimization theory covered, as well as econometric and economic theory, to solve a profit maximization problem.\n",
    "\n",
    "\n",
    "Suppose, as a data scientist working for your company, you are tasked with estimating the optimal amount of money to allocate towards different advertising channels that will maximize the overall profit of a certain product line. Furthermore, suppose you are given constraints on these allocation decisions, such as the maximum total spend you have to allocate and/or minimum amounts that have to spent in certain channels. In this article, we are going to combine the optimization theory covered in part 1 and part 2 of this series, along with additional economic and econometric theory to tackle a theoretical profit maximization problem of this sorts — which we will flesh out in more detail in this article.\n",
    "\n",
    "The goal of this article is to tie together what we have learned thus far and my hope is to motivate and inspire readers on how to incorporate these techniques into an applied setting. It is not meant to be a comprehensive solution to the problem covered as nuances and idiosyncrasies can, of course, complicate theoretical examples. Furthermore, many of the techniques covered have much more optimized implementations in python via packages such as [pyomo](https://www.pyomo.org/), [SciPy](https://docs.scipy.org/doc/scipy/tutorial/optimize.html), etc. Nevertheless, I hope to provide a strong framework for constructing applied optimization problems. Let’s dive into it!\n",
    "\n",
    "## Optimization Theory - Parts 1 & 2 Recap\n",
    "> **Brief Recap:** In part 1, we covered basic optimization theory — including 1) setting up and solving a simple single variable optimization problem analytically, 2) iterative optimization schemes — namely, gradient descent & Newton’s Method, and 3) implementing Newton’s method by hand and in python for a multi-dimensional optimization problem. In part 2, we covered constrained optimization theory — including 1) incorporating equality constraints and 2) incorporating inequality constraints into our optimization problems and solving them via Newton’s Method. This article is designed to be accessible for those who are already familiar with the content covered in part 1 and part 2.\n",
    "\n",
    "A mathematical optimization problem can be formulated abstractly as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{x}} \\quad& f(\\mathbf{x}), \\mathbf{x}=[x_1,x_2,\\dots,x_n]^T \\in \\mathbb{R}^n \\\\\n",
    "\\text{subject to} \\quad& g_j(\\mathbf{x}) \\le 0, j=1,2,\\dots,m \\\\\n",
    "& h_j(\\mathbf{x}) = 0, j=1,2,\\dots,r\n",
    "\\end{aligned}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we choose real values of the vector $\\mathbf{x}$ that minimize the objective function $f(x)$ (or maximize $-f(x)$) subject to the inequality constraints $g(x)$ and equality constraints $h(x)$. In part 2, we discussed how to incorporate these constraints directly into our optimization problem. Notably, using Lagrange Multipliers and Logarithmic Barrier functions we can construct a new objective function $\\mathcal{O}(\\mathbf{x}, \\Lambda, \\rho)$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{O}\n",
    "(\\mathbf{x},\\Lambda,\\rho) &= f(\\mathbf{x}) + \\sum^r_{j=1}\\lambda_jh_j(\\mathbf{x})- \\rho\\sum^m_{j=1}\\log(c_j(\\mathbf{x})), \\\\\n",
    "\\mathbf{x}&=[x_1,x_2,\\dots,x_n] \\\\[6pt]\n",
    "\\Lambda &= [\\lambda_1,\\lambda_2,\\dots,\\lambda_r] \\\\[6pt]\n",
    "c_j(\\mathbf{x}) &= \\begin{cases}\n",
    "g_j(\\mathbf{x}), & g_j(\\mathbf{x}) \\geq 0 \\\\\n",
    "-g_j(\\mathbf{x}), & g_j(\\mathbf{x}) < 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ is the vector of Lagrange multipliers associated with each equality constraints $h(x)$ and $\\rho$ is the barrier parameter associated with all of the inequality constraints $g(x)$. We can then solve this new objective function iterating by choose a starting value $\\rho$ (note that large functional values of the objective function will require much larger starting values of $\\rho$ to scale the penalization), optimize the new objective function evaluated at $\\rho$ using Newton’s method iterative scheme, then update $\\rho$ by slowly decreasing it ($\\rho \\rightarrow 0$), and repeat until convergence — where Newton’s Method iterative scheme is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k -\\mathbf{H}^{-1}(\\mathbf{x}_k)\\nabla f(\\mathbf{x}_k)\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H}(\\mathbf{x})$ and $\\nabla f(x)$ denote the Hessian and gradient of our objective function $\\mathcal{O}(\\mathbf{x}, \\Lambda, \\rho)$, respectively. Convergence is obtained when we reach convergence across one or more of the following criteria:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\text{Criteria 1: } \\lVert \\mathbf{x}_k - \\mathbf{x}_{k-1} \\rVert < \\epsilon_1 \\\\[6pt]\n",
    "&\\text{Criteria 2: } \\lvert f(\\mathbf{x}_k) - f(\\mathbf{x}_{k-1}) \\rvert < \\epsilon_2\n",
    "\\end{aligned}\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In python, utilizing [SymPy](https://www.sympy.org/en/index.html), we have 4 functions. A function that obtains the gradient of our SymPy function, the Hessian of our SymPy function, solves unconstrained optimization problem via Newton’s method, and solves a constrained optimization problem via Newton’s method according to the generalization eq. (2).\n",
    "\n",
    "To solve a constrained optimization problem, we can run the following code (Make sure starting values are in the feasible range of inequality constraints!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Functions constructed in Part 1 + Part 2\n",
    "\n",
    "\n",
    "def get_gradient(\n",
    "    function: sm.Expr,\n",
    "    symbols: list[sm.Symbol],\n",
    "    x0: dict[sm.Symbol, float],  # Add x0 as argument\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the gradient of a function at a given point.\n",
    "\n",
    "    Args:\n",
    "        function (sm.Expr): The function to calculate the gradient of.\n",
    "        symbols (list[sm.Symbol]): The symbols representing the variables in the function.\n",
    "        x0 (dict[sm.Symbol, float]): The point at which to calculate the gradient.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The gradient of the function at the given point.\n",
    "    \"\"\"\n",
    "    d1 = {}\n",
    "    gradient = np.array([])\n",
    "\n",
    "    for i in symbols:\n",
    "        d1[i] = sm.diff(function, i, 1).evalf(subs=x0)  # add evalf method\n",
    "        gradient = np.append(gradient, d1[i])\n",
    "\n",
    "    return gradient.astype(np.float64)  # Change data type to float\n",
    "\n",
    "\n",
    "def get_hessian(\n",
    "    function: sm.Expr,\n",
    "    symbols: list[sm.Symbol],\n",
    "    x0: dict[sm.Symbol, float],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Hessian matrix of a function at a given point.\n",
    "\n",
    "    Args:\n",
    "    function (sm.Expr): The function for which the Hessian matrix is calculated.\n",
    "    symbols (list[sm.Symbol]): The list of symbols used in the function.\n",
    "    x0 (dict[sm.Symbol, float]): The point at which the Hessian matrix is evaluated.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The Hessian matrix of the function at the given point.\n",
    "    \"\"\"\n",
    "    d2 = {}\n",
    "    hessian = np.array([])\n",
    "\n",
    "    for i in symbols:\n",
    "        for j in symbols:\n",
    "            d2[f\"{i}{j}\"] = sm.diff(function, i, j).evalf(subs=x0)\n",
    "            hessian = np.append(hessian, d2[f\"{i}{j}\"])\n",
    "\n",
    "    hessian = np.array(np.array_split(hessian, len(symbols)))\n",
    "\n",
    "    return hessian.astype(np.float64)\n",
    "\n",
    "\n",
    "def newtons_method(\n",
    "    function: sm.Expr,\n",
    "    symbols: list[sm.Symbol],\n",
    "    x0: dict[sm.Symbol, float],\n",
    "    iterations: int = 100,\n",
    "    tolerance: float = 10e-5,\n",
    "    verbose: int = 1,\n",
    ") -> dict[sm.Symbol, float] or None:\n",
    "    \"\"\"\n",
    "    Perform Newton's method to find the solution to the optimization problem.\n",
    "\n",
    "    Args:\n",
    "        function (sm.Expr): The objective function to be optimized.\n",
    "        symbols (list[sm.Symbol]): The symbols used in the objective function.\n",
    "        x0 (dict[sm.Symbol, float]): The initial values for the symbols.\n",
    "        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n",
    "        tolerance (float, optional): Threshold for determining convergence.\n",
    "        verbose (int, optional): Control verbosity of output. 0 is no output, 1 is full output.\n",
    "\n",
    "    Returns:\n",
    "        dict[sm.Symbol, float] or None: The solution to the optimization problem, or None if no solution is found.\n",
    "    \"\"\"\n",
    "\n",
    "    x_star = {}\n",
    "    x_star[0] = np.array(list(x0.values()))\n",
    "\n",
    "    if verbose != 0:\n",
    "        print(f\"Starting Values: {x_star[0]}\")\n",
    "\n",
    "    for i in range(iterations):\n",
    "        gradient = get_gradient(function, symbols, dict(zip(x0.keys(), x_star[i])))\n",
    "        hessian = get_hessian(function, symbols, dict(zip(x0.keys(), x_star[i])))\n",
    "\n",
    "        x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n",
    "\n",
    "        if np.linalg.norm(x_star[i + 1] - x_star[i]) < tolerance:\n",
    "            solution = dict(zip(x0.keys(), [float(x) for x in x_star[i + 1]]))\n",
    "            if verbose != 0:\n",
    "                print(\n",
    "                    f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\"\n",
    "                )\n",
    "            break\n",
    "        else:\n",
    "            solution = None\n",
    "\n",
    "        if verbose != 0:\n",
    "            print(f\"Step {i+1}: {x_star[i+1]}\")\n",
    "\n",
    "    return solution\n",
    "\n",
    "\n",
    "def constrained_newtons_method(\n",
    "    function: sm.Expr,\n",
    "    symbols: list[sm.Symbol],\n",
    "    x0: dict[sm.Symbol, float],\n",
    "    rho_steps: int = 100,\n",
    "    discount_rate: float = 0.9,\n",
    "    newton_method_iterations: int = 100,\n",
    "    tolerance: float = 10e-5,\n",
    ") -> dict[sm.Symbol, float] | None:\n",
    "    \"\"\"\n",
    "    Performs constrained Newton's method to find the optimal solution of a function subject to constraints.\n",
    "\n",
    "    Args:\n",
    "        function (sm.Expr): The function to optimize.\n",
    "        symbols (list[sm.Symbol]): The symbols used in the function.\n",
    "        x0 (dict[sm.Symbol, float]): The initial values for the symbols.\n",
    "        rho_steps (int, optional): The number of steps to update rho. Defaults to 100.\n",
    "        discount_rate (float, optional): The scalar to discount rho by at each step. Default is 0.9.\n",
    "        newton_method_iterations (int, optional): The maximum number of iterations in Newton Method internal loop. Defaults to 100.\n",
    "        tolerance (float, optional): Threshold for determining convergence.\n",
    "\n",
    "    Returns:\n",
    "        dict[sm.Symbol, float] or None: The optimal solution if convergence is achieved, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    rho = list(x0.keys())[-1]\n",
    "    optimal_solutions = []\n",
    "    optimal_solutions.append(x0)\n",
    "\n",
    "    for step in range(rho_steps):\n",
    "        if step % 10 == 0:\n",
    "            print(\"\\n\" + \"===\" * 20)\n",
    "            print(f\"Step {step} w/ rho={optimal_solutions[step][rho]}\")\n",
    "            print(\"===\" * 20 + \"\\n\")\n",
    "            print(f\"Current solution: {optimal_solutions[step]}\")\n",
    "\n",
    "        function_eval = function.evalf(subs={rho: optimal_solutions[step][rho]})\n",
    "\n",
    "        values = optimal_solutions[step].copy()\n",
    "        del values[rho]\n",
    "\n",
    "        optimal_solution = newtons_method(\n",
    "            function_eval,\n",
    "            symbols[:-1],\n",
    "            values,\n",
    "            iterations=newton_method_iterations,\n",
    "            tolerance=tolerance,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        optimal_solutions.append(optimal_solution)\n",
    "\n",
    "        # Check for overall convergence\n",
    "        current_solution = np.array(\n",
    "            [v for k, v in optimal_solutions[step].items() if k != rho]\n",
    "        )\n",
    "        previous_solution = np.array(\n",
    "            [v for k, v in optimal_solutions[step - 1].items() if k != rho]\n",
    "        )\n",
    "        if np.linalg.norm(current_solution - previous_solution) < tolerance:\n",
    "            overall_solution = optimal_solutions[step]\n",
    "            del overall_solution[rho]\n",
    "            print(\n",
    "                f\"\\n Overall Convergence Achieved ({step} steps): Solution = {overall_solution}\\n\"\n",
    "            )\n",
    "            break\n",
    "        else:\n",
    "            overall_solution = None\n",
    "\n",
    "        # Update rho\n",
    "        optimal_solutions[step + 1][rho] = discount_rate * optimal_solutions[step][rho]\n",
    "\n",
    "    return overall_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_rosenbrocks():\n",
    "    x, y, λ, ρ = sm.symbols(\"x y λ ρ\")\n",
    "\n",
    "    # f(x): 100*(y-x**2)**2 + (1-x)**2\n",
    "    # h(x): x**2 - y = 2\n",
    "    # g_1(x): x <= 0\n",
    "    # g_2(x) y >= 3\n",
    "\n",
    "    combined_objective = (\n",
    "        100 * (y - x**2) ** 2\n",
    "        + (1 - x) ** 2\n",
    "        + λ * (x**2 - y - 2)\n",
    "        - ρ * sm.log((-x) * (y - 3))\n",
    "    )\n",
    "    Gamma = [x, y, λ, ρ]  # Function requires last symbol to be ρ!\n",
    "    Gamma0 = {x: -15, y: 15, λ: 0, ρ: 10}\n",
    "\n",
    "    return constrained_newtons_method(combined_objective, Gamma, Gamma0)\n",
    "\n",
    "\n",
    "_ = combined_rosenbrocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "If the material above felt foreign or you need a more rigorous recap, then I recommend taking a look at part 1 and part 2 of this series which will provide a more in-depth survey of the material above. For the remainder of this article, we will first discuss basic profit maximization & econometric theory and then move into solving the theoretical example.\n",
    "\n",
    "## Applied Profit Maximization\n",
    "\n",
    "> **Problem:** Suppose we have a $100,000 advertising budget and all of it must be spent. We are tasked with choosing the optimal amount of this budget to allocate towards two types of advertisement channels (digital ads and television ads) that maximize the overall profit for a particular product line. Furthermore, suppose that we must allocate at a minimum of $20k to television advertising and $10k to digital advertising.\n",
    "\n",
    "### Theoretical Formulation\n",
    "\n",
    "Let's now mathematically formulate the profit maximization problem we seek to solve:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\delta,\\tau} \\quad& -\\pi(\\delta,\\tau,\\cdot) \\\\\n",
    "\\text{subject to} \\quad& \\textbf{Budget Constraint: } \\delta + \\tau = \\text{\\$100,000} \\\\\n",
    "&\\textbf{Minimum Requirements: } \\delta \\ge \\text{\\$10,000}, \\text{ } \\tau \\ge \\text{\\$20,000} \\\\\n",
    "\\end{aligned}\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\pi(\\cdot)$ denotes the profit function, $\\delta$ denotes digital ad spend, $\\tau$ denotes television ad spend, and $(\\cdot)$ is a placeholder for additional variables. Note that we are minimizing the negative of $\\pi(\\cdot)$ which is equivalent to maximizing $\\pi(\\cdot)$. The profit function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\pi(\\delta,\\tau,\\cdot) &= \\text{Revenue}-\\text{Cost} \\\\\n",
    "&= p\\times q(\\delta,\\tau,\\cdot)-\\mathcal{C}\\bigl[q(\\delta,\\tau,\\cdot),\\delta,\\tau\\bigr]\n",
    "\\end{aligned}\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $p$ denotes the price, $q(\\delta, \\tau, \\cdot)$ denotes the quantity demanded function, and $\\mathcal{C}(q(\\cdot), \\delta, \\tau)$ denotes the cost function which, intuitively, is a direct function of the quantity (if we make more it will cost more to produce) and how much we spend on advertising. The cost function can also take on additional inputs, but for the sake of demonstration we will keep it as a function of quantity and advertising costs. Notice that our choices of $\\delta$ and $\\tau$ impact the profit function directly through their impact of quantity demanded and the cost function. In order to add tractability to our optimization problem, we will need to use econometric techniques to estimate our quantity function. Once we have specified our cost function and estimated the quantity function, we can then solve our optimization problem as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\delta,\\tau} \\quad& - \\left\\{ p\\times \\hat{q}(\\delta,\\tau,\\cdot)-\\mathcal{C}\\bigl[\\hat{q}(\\delta,\\tau,\\cdot),\\delta,\\tau\\bigr] \\right\\} \\\\\n",
    "\\text{subject to} \\quad& \\textbf{Budget Constraint: } \\delta + \\tau = \\text{\\$100,000} \\\\\n",
    "&\\textbf{Minimum Requirements: } \\delta \\ge \\text{\\$10,000}, \\text{ } \\tau \\ge \\text{\\$20,000} \\\\\n",
    "\\end{aligned}\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\hat{q}$ is our estimated econometric model for quantity demanded. Before we lay out the econometric specification of our quantity model, it is necessary that we discuss an important note regarding the required assumptions for this optimization problem to prove tractable. It is imperative that we obtain the causal estimates of digital and television advertising on the quantity demanded. In the economists jargon, digital and television advertising need be exogenous in the econometric model. That is, they are uncorrelated with the error in the model. Exogeneity can be achieved in two ways: 1) We have the correct structural specification of the econometric model for the impact of digital and television advertising on quantity demanded (i.e., we include all of the relevant variables that are correlated with both quantity demanded and digital & television advertising spend) or 2) We have random variation of digital & television advertising spend (this can be achieved from randomly varying spend over time to see how demand responds).\n",
    "\n",
    "Intuitively, exogeneity is required because it is necessary to capture the causal impact of changing advertising spend — that is, what will happen, on average, if we change the values of the advertising spend. If the effect we estimate is not causal then the changes we make in advertising spend will not correspond to the true change in quantity demanded. Note the model need not make the best predictions for quantity demanded, but rather accurately capture the causal relationship.\n",
    "\n",
    "Let’s now suppose we specify the following econometric model for quantity demanded indexed by time t:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "q_t(\\delta,\\tau,\\cdot)=\\alpha+\\beta\\ln(\\delta_t)+\\gamma\\ln(\\tau_t)+\\phi_1q_{t-1}+\\phi_2q_{t-2}+\\mathcal{S}_t+\\mathbf{ X}_t\\Omega+\\epsilon\n",
    "\\tag{8}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\beta$ and $\\gamma$ are the estimates of the impact of the natural log of digital ad spend, $\\delta$, and television ad spend, $\\tau$, respectively. Additionally, $\\alpha$ is our intercept, $\\phi_1$ and $\\phi_2$ are estimates of the autoregressive components of quantity demanded, $\\mathcal{S}$ denotes seasonality, $\\mathbf{X}$ is the set of all relevant covariates and lagged covariates along with the matrix of their coefficient estimates $\\Omega$, and $\\epsilon$ is the error term. Furthermore, assume that digital and television advertising satisfy the exogeneity assumption conditional on $\\mathbf{X}$, $\\mathcal{S}$, and the autoregressive components within our model. That is,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Cov}[\\ln(\\delta),\\epsilon|{\\bf X},\\mathcal{S},q_{t-1},q_{t-2}]=\\mathrm{Cov}[\\ln({\\tau}),\\epsilon|{\\bf X},\\mathcal{S},q_{t-1},q_{t-2}]=0\n",
    "\\tag{9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Why the natural log of digital and television ad spend you may ask? This is by no means a required nor a definitive decision in this context, but I am seeking to demonstrate how variable transformations can capture hypotheses about the relationship between our choice variables and the outcomes of interest. In our case, suppose we hypothesize that the impact on ad spend increases sharply initially, but gradually levels out (e.g., saturation effects or the law of diminishing returns). This is exactly what the logarithm transformation will allow us to model. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def saturation_plot():\n",
    "    fig, ax = plt.subplots(dpi=125)\n",
    "\n",
    "    x = np.linspace(5, 1000, 1000)\n",
    "\n",
    "    y = np.log(x)\n",
    "\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "    plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "    plt.tick_params(\n",
    "        axis=\"y\", which=\"both\", bottom=False, top=False, left=False, labelleft=False\n",
    "    )\n",
    "\n",
    "    plt.plot(x, y, color=\"g\")\n",
    "    plt.xlabel(\"Digital Advertising\", size=\"large\")\n",
    "    plt.ylabel(\"Quantity Demanded\", size=\"large\")\n",
    "\n",
    "    plt.savefig(\n",
    "        \"data/saturation_plot.webp\", format=\"webp\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "\n",
    "saturation_plot()\n",
    "mo.image(\"data/saturation_plot.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "Note that the cost functional form is generally known in advance. Thus, let’s specify the functional form of our cost function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{C}\\bigl[q(\\delta,\\tau,\\cdot),\\delta,\\tau\\bigr]=q(\\delta,\\tau,\\cdot)\\times\\bigl[\\zeta-\\text{discount}\\times q(\\delta,\\tau,\\cdot)\\bigr] + \\delta+\\tau\n",
    "\\tag{10}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here we can see that we have a cost $\\zeta$ associated with each unit produced and this cost is discounted as we produce more (think a discount for larger contracts or [economies of scale](https://en.wikipedia.org/wiki/Economies_of_scale)). We also simply sum digital ad spend and television ad spend into our total cost.\n",
    "\n",
    "Now that we have developed the theoretical basis for our econometric profit maximization problem, let’s simulate some data and take this to python!\n",
    "\n",
    "### Optional: Data Simulation\n",
    "\n",
    "Note this section can be skipped without any loss of the primary content.\n",
    "\n",
    "Let’s first simulate monthly data over 10 years for quantity demanded, where the following variables included are as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "## Digital Advertising - ln(δ)\n",
    "df[\"log_digital_advertising\"] = np.log(\n",
    "    np.random.normal(loc=50000, scale=15000, size=120).round()\n",
    ")\n",
    "\n",
    "## Television Advertising - ln(τ)\n",
    "df[\"log_television_advertising\"] = np.log(\n",
    "    np.random.normal(loc=50000, scale=15000, size=120).round()\n",
    ")\n",
    "\n",
    "## Matrix X of covariates\n",
    "\n",
    "# Lag Digital Advertising\n",
    "df[\"log_digital_advertising_lag1\"] = df[\"log_digital_advertising\"].shift(1)\n",
    "df[\"log_digital_advertising_lag2\"] = df[\"log_digital_advertising\"].shift(2)\n",
    "\n",
    "# Lag Television Advertising\n",
    "df[\"log_television_advertising_lag1\"] = df[\"log_television_advertising\"].shift(1)\n",
    "df[\"log_television_advertising_lag2\"] = df[\"log_television_advertising\"].shift(2)\n",
    "\n",
    "# Price\n",
    "df[\"price\"] = np.random.normal(loc=180, scale=15, size=120).round()\n",
    "df[\"price_lag1\"] = df[\"price\"].shift(1)\n",
    "df[\"price_lag2\"] = df[\"price\"].shift(2)\n",
    "\n",
    "# Competitor Price\n",
    "df[\"comp_price\"] = np.random.normal(loc=120, scale=15, size=120).round()\n",
    "df[\"comp_price_lag1\"] = df[\"comp_price\"].shift(1)\n",
    "df[\"comp_price_lag2\"] = df[\"comp_price\"].shift(2)\n",
    "\n",
    "# Seasonality\n",
    "months = cycle(\n",
    "    [\n",
    "        \"Jan\",\n",
    "        \"Feb\",\n",
    "        \"Mar\",\n",
    "        \"Apr\",\n",
    "        \"May\",\n",
    "        \"June\",\n",
    "        \"July\",\n",
    "        \"Aug\",\n",
    "        \"Sep\",\n",
    "        \"Oct\",\n",
    "        \"Nov\",\n",
    "        \"Dec\",\n",
    "    ]\n",
    ")\n",
    "df[\"months\"] = [next(months) for m in range(len(df))]\n",
    "\n",
    "one_hot = pd.get_dummies(df[\"months\"], dtype=int)\n",
    "one_hot = one_hot[\n",
    "    [\n",
    "        \"Jan\",\n",
    "        \"Feb\",\n",
    "        \"Mar\",\n",
    "        \"Apr\",\n",
    "        \"May\",\n",
    "        \"June\",\n",
    "        \"July\",\n",
    "        \"Aug\",\n",
    "        \"Sep\",\n",
    "        \"Oct\",\n",
    "        \"Nov\",\n",
    "        \"Dec\",\n",
    "    ]\n",
    "]\n",
    "df = df.join(one_hot).drop(\"months\", axis=1)\n",
    "\n",
    "## Constant\n",
    "df[\"constant\"] = 1\n",
    "\n",
    "# Drop NaN (Two lags)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "Note that we include lag variables because it is highly plausible that today’s quantity demanded is a function of lagged values for many of the variables. We also control for seasonality effects by incorporation dummy variables for each month (this is one of many ways to incorporate seasonality into the model). We then specify the parameters associated with each variable as (note that these parameters are specified in the same order as the columns of the dataframe!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.array(\n",
    "    [\n",
    "        10_000,  # β\n",
    "        5_000,  # γ\n",
    "        2_000,  # Ω\n",
    "        1_000,  # Ω\n",
    "        3_000,  # Ω\n",
    "        1_000,  # Ω\n",
    "        -1_000,  # Ω\n",
    "        -500,  # Ω\n",
    "        -100,  # Ω\n",
    "        500,  # Ω\n",
    "        300,  # Ω\n",
    "        100,  # Ω\n",
    "        25_000,  # S\n",
    "        15_000,  # S\n",
    "        15_000,  # S\n",
    "        10_000,  # S\n",
    "        10_000,  # S\n",
    "        10_000,  # S\n",
    "        15_000,  # S\n",
    "        15_000,  # S\n",
    "        25_000,  # S\n",
    "        35_000,  # S\n",
    "        35_000,  # S\n",
    "        40_000,  # S\n",
    "        50_000,  # α\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "We can then simulate our econometric specification (eq. 8) of quantity demanded by running `quantity_demanded = np.array(df) @ params`. However, note that we are missing the autoregressive components, thus we also want quantity demanded to follow an autoregressive process as mentioned above. That is, quantity demanded is also a function of its own lagged values. We include 2 lags here (AR(2) process) with respective coefficients $\\phi_1$ and $\\phi_2$. Note, we can simulate this with initial conditions $q_0$ and $q_{-1}$ via the following system:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "q_1 \\\\\n",
    "q_2 \\\\\n",
    "q_3 \\\\\n",
    "q_4 \\\\\n",
    "\\vdots \\\\\n",
    "q_t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 &  0 & \\dots & 0 & 0 & 0 \\\\\n",
    "-\\phi_1 & 1 & 0 & 0 & \\dots & 0 & 0 & 0 \\\\\n",
    "-\\phi_2 & -\\phi_1 & 1 & 0 & \\dots & 0 & 0 & 0 \\\\\n",
    "0 & -\\phi_2 & -\\phi_1 & 1 &  \\dots & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\dots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & \\dots & -\\phi_2 & -\\phi_1 & 1\n",
    "\\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix}\n",
    "\\alpha+\\beta\\ln(\\delta_1)+\\gamma\\ln(\\tau_1)+\\mathbf{X}_1\\mathbf{\\Omega}+\\epsilon_1 + (\\phi_1q_0+\\phi_2q_{-1})  \\\\\n",
    "\\alpha+\\beta\\ln(\\delta_2)+\\gamma\\ln(\\tau_2)+\\mathbf{X}_2\\mathbf{\\Omega}+\\epsilon_2 + (\\phi_2q_0) \\\\\n",
    "\\alpha+\\beta\\ln(\\delta_3)+\\gamma\\ln(\\tau_3)+\\mathbf{X}_3\\mathbf{\\Omega}+\\epsilon_3 \\\\\n",
    "\\alpha+\\beta\\ln(\\delta_4)+\\gamma\\ln(\\tau_4)+\\mathbf{X}_4\\mathbf{\\Omega}+\\epsilon_4 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha+\\beta\\ln(\\delta_t)+\\gamma\\ln(\\tau_t)+\\mathbf{X}_t\\mathbf{\\Omega}+\\epsilon_t \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantity_ar2_process(T, ϕ1, ϕ2, q0, q_1, ϵ, df, params):\n",
    "    Φ = np.identity(T)  # The T x T identity matrix\n",
    "\n",
    "    for i in range(T):\n",
    "        if i - 1 >= 0:\n",
    "            Φ[i, i - 1] = -ϕ1\n",
    "\n",
    "        if i - 2 >= 0:\n",
    "            Φ[i, i - 2] = -ϕ2\n",
    "\n",
    "    B = np.array(df) @ params + ϵ\n",
    "\n",
    "    B[0] = B[0] + ϕ1 * q0 + ϕ2 * q_1\n",
    "    B[1] = B[1] + ϕ2 * q0\n",
    "\n",
    "    return np.linalg.inv(Φ) @ B\n",
    "\n",
    "\n",
    "## Quantity Demand AR(2) component process\n",
    "\n",
    "# Parameters\n",
    "T = 118  # Time periods less two lags\n",
    "ϕ1 = 0.3  # Lag 1 coefficient (ϕ1)\n",
    "ϕ2 = 0.05  # Lag 2 coefficient (ϕ2)\n",
    "q_1 = 250_000  # Initial Condition q_-1\n",
    "q0 = 300_000  # Initial Condition q_0\n",
    "ϵ = np.random.normal(0, 5000, size=T)  # Random Error (ϵ)\n",
    "\n",
    "quantity_demanded_ar = quantity_ar2_process(T, ϕ1, ϕ2, q0, q_1, ϵ, df, params)\n",
    "\n",
    "# Quantity_demanded target variable\n",
    "df[\"quantity_demanded\"] = quantity_demanded_ar\n",
    "\n",
    "# Additional covariates of lagged quantity demanded\n",
    "df[\"quantity_demanded_lag1\"] = df[\"quantity_demanded\"].shift(1)\n",
    "df[\"quantity_demanded_lag2\"] = df[\"quantity_demanded\"].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "### Econometric Estimation & Profit Maximization\n",
    "\n",
    "Let’s first use our framework in eq. (2) to transform our constrained optimization problem in eq. (7) to one in which we can solve utilizing our function `constrained_newton_method()` as done above:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{O}(\\delta,\\tau,\\lambda,\\rho)= -\\Bigl\\{ p\\times \\hat{q}(\\delta,\\tau,\\cdot)-\\mathcal{C}\\bigl[\\hat{q}(\\delta,\\tau,\\cdot),\\delta,\\tau\\bigr] \\Bigr\\} \\\\\n",
    "+\\lambda(\\delta+\\tau-\\text{100,000}) \\\\\n",
    "-\\rho\\log\\bigl[(\\tau-\\text{20,000})(\\delta-\\text{10,000})\\bigr]\n",
    "\\end{aligned}\n",
    "\\tag{12}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As discussed before, we need to estimate our quantity demanded, $\\hat{q}$. Let’s take a look at what our quantity demanded looks like over the 10 years simulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def quantity_demanded_plot():\n",
    "    fig, ax = plt.subplots(dpi=125)\n",
    "\n",
    "    fmt = \"{x:,.0f}\"\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(tick)\n",
    "\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    # ax.spines['bottom'].set_color('none')\n",
    "    # ax.spines['bottom'].set_position(('data',avg_demand))\n",
    "\n",
    "    plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "    for i in range(0, 108, 12):\n",
    "        line = 108 - i\n",
    "        ax.axvline(x=line, color=\"r\", linestyle=\"--\", alpha=0.2)\n",
    "\n",
    "    # ax.axhline(y=avg_demand, color='black', linewidth=0.2)\n",
    "\n",
    "    plt.plot(np.arange(T) + 1, quantity_demanded_ar, color=\"g\")\n",
    "    plt.xlabel(\"Years\", size=\"large\")\n",
    "    plt.ylabel(\"Quantity Demanded\", size=\"large\")\n",
    "\n",
    "    plt.savefig(\n",
    "        \"data/quantity_demanded_plot.webp\", format=\"webp\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "\n",
    "quantity_demanded_plot()\n",
    "mo.image(\"data/quantity_demanded_plot.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "We can clearly see some seasonality occurring towards the end of the years and it appears we are dealing with a [stationary process](https://en.wikipedia.org/wiki/Stationary_process) (this is all by construction). Now suppose that we have the following observed variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "where, in eq. 8, our econometric specification, quantity_demanded is our outcome $q$, log_digital_advertising is our $\\ln(\\delta)$, log_television_advertising is our $\\ln(\\tau)$, constant is our $α$, quantity_demanded_lag1 & quantity_demanded_lag2 are our autoregressive components $q_{t-1}$ & $q_{t-2}$, and the remainder are our additional covariates $\\mathbf{X}$ including seasonality $\\mathcal{S}$.\n",
    "\n",
    "Now, with this data, we seek to estimate our econometric specification in eq. 8. We can estimate this structural model using OLS. For this we will use [statsmodels](https://www.statsmodels.org/stable/index.html#).\n",
    "\n",
    "> A great exercise would be to solve the linear regression using the Gradient Descent ot Newton’s method code we have constructed and compare the results to statsmodels. Hint: the objective in a linear regression is to minimize the Residual Sum of Squares. Note that the code we have written is by no means an efficient approach to solving a linear regression, but this is more oriented towards illustrating optimization theory in a model fitting (regression) context. Code for this will be provided at the end of the article!\n",
    "\n",
    "Note that we drop the first 2 observations as these are our first two lags and we drop July as a reference month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit Econometric model using OLS\n",
    "df_mod = df[2:]  # Drop first two lagged values\n",
    "\n",
    "y = df_mod[\"quantity_demanded\"]\n",
    "X = df_mod.drop([\"quantity_demanded\", \"July\"], axis=1)\n",
    "\n",
    "mod = stats.OLS(y, X)\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "Now we have our estimated econometric specification for quantity demanded! A few observations:\n",
    "\n",
    "1. An increase in log digital ad spend and log television ad spend are associated with an increase in quantity demand\n",
    "2. An increase price is associated with a decrease in quantity demand (this is expected behavior)\n",
    "3. We see clear seasonality with increasing demand during Sep-Dec, this is consistent with our time series above\n",
    "4. We see that the first lag of quantity demanded is predictive of the present, in favor of autoregressive process\n",
    "\n",
    "> The results above can be verified and compared with the data construction above in the data simulation section\n",
    "\n",
    "Let’s now specify our symbolic variables for our optimization problem ($\\delta$, $\\tau$, $\\lambda$, and $\\rho$), the values of our present variables at time $t$, and grab the lagged values from our data. We can then obtain $\\hat{q}(\\delta, \\tau)$ at a point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Symbolic Functions with all variables in function\n",
    "δ, τ, λ, ρ = sm.symbols(\"δ τ λ ρ\")\n",
    "\n",
    "## Values of current variables\n",
    "price = 180\n",
    "comp_price = 120\n",
    "Jan = 1\n",
    "\n",
    "## Obtain Lagged Values\n",
    "log_digital_advertising_lag1 = df[\"log_digital_advertising\"].iloc[-1]\n",
    "log_digital_advertising_lag2 = df[\"log_digital_advertising\"].iloc[-2]\n",
    "log_television_advertising_lag1 = df[\"log_television_advertising\"].iloc[-1]\n",
    "log_television_advertising_lag2 = df[\"log_television_advertising\"].iloc[-2]\n",
    "price_lag1 = df[\"price\"].iloc[-1]\n",
    "price_lag2 = df[\"price\"].iloc[-2]\n",
    "comp_price_lag1 = df[\"comp_price\"].iloc[-1]\n",
    "comp_price_lag2 = df[\"comp_price\"].iloc[-2]\n",
    "quantity_demanded_lag1 = df[\"quantity_demanded\"].iloc[-1]\n",
    "quantity_demanded_lag2 = df[\"quantity_demanded\"].iloc[-2]\n",
    "\n",
    "variables = [\n",
    "    sm.log(δ),\n",
    "    sm.log(τ),\n",
    "    log_digital_advertising_lag1,\n",
    "    log_digital_advertising_lag2,\n",
    "    log_television_advertising_lag1,\n",
    "    log_television_advertising_lag2,\n",
    "    price,\n",
    "    price_lag1,\n",
    "    price_lag2,\n",
    "    comp_price,\n",
    "    comp_price_lag1,\n",
    "    comp_price_lag2,\n",
    "    Jan,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,  # All Months less July\n",
    "    1,  # Constant\n",
    "    quantity_demanded_lag1,\n",
    "    quantity_demanded_lag2,\n",
    "]\n",
    "\n",
    "# Quantity Demanded\n",
    "quantity_demanded = (np.array([variables]) @ np.array(results.params))[\n",
    "    0\n",
    "]  # params from ols model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvdt",
   "metadata": {},
   "source": [
    "$\\hat{q}(\\delta,\\tau,\\cdot) = 10071.2795746647*\\log(δ) + 6219.99261508067*\\log(τ) + 21336.8117838209$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {},
   "source": [
    "Now we can construct our revenue, cost, and put them together to construct our profit function. Here our cost to produce each unit is $140 base and is discounted by $0.0001 for each additional unit produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "Revenue = price * quantity_demanded\n",
    "Cost = quantity_demanded * (140 - 0.0001 * quantity_demanded) + τ + δ\n",
    "profit = Revenue - Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "$\\pi(\\delta,\\tau,\\cdot) = -δ - τ - (-1.00712795746647*\\log(δ) - 0.621999261508067*\\log(τ) + 137.866318821618)*(10071.2795746647*\\log(δ) + 6219.99261508067*\\log(τ) + 21336.8117838209) + 1812830.32343965*\\log(δ) + 1119598.67071452*\\log(τ) + 3840626.12108775$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXTn",
   "metadata": {},
   "source": [
    "Plotting our profit as a function of digital ad spend and television ad spend, $π(\\delta, \\tau)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def profit_function_viz_3d():\n",
    "    def log(x):\n",
    "        return np.log(x)\n",
    "\n",
    "    # Create meshgrid for surface\n",
    "    δ_vals = np.linspace(0.01, 100000, 100)\n",
    "    τ_vals = np.linspace(0.01, 100000, 100)\n",
    "    δ_mesh, τ_mesh = np.meshgrid(δ_vals, τ_vals)\n",
    "\n",
    "    def profit_fn(δ, τ):\n",
    "        return (\n",
    "            -δ\n",
    "            - τ\n",
    "            - (\n",
    "                -1.00712795746647 * log(δ)\n",
    "                - 0.621999261508067 * log(τ)\n",
    "                + 137.866318821618\n",
    "            )\n",
    "            * (10071.2795746647 * log(δ) + 6219.99261508067 * log(τ) + 21336.8117838209)\n",
    "            + 1812830.32343965 * log(δ)\n",
    "            + 1119598.67071452 * log(τ)\n",
    "            + 3840626.12108775\n",
    "        )\n",
    "\n",
    "    # Calculate profit values\n",
    "    profit_vals = profit_fn(δ_mesh, τ_mesh)\n",
    "\n",
    "    # Create surface plot\n",
    "    surface = go.Surface(\n",
    "        x=δ_mesh.tolist(),\n",
    "        y=τ_mesh.tolist(),\n",
    "        z=profit_vals.tolist(),\n",
    "        colorscale=\"plasma\",\n",
    "        name=\"Profit Surface\",\n",
    "        colorbar=dict(x=-0.15),\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Create budget constraint line\n",
    "    budget_x = np.linspace(0.01, 99_999, 100)\n",
    "    budget_y = 100000 - budget_x\n",
    "    budget_z = profit_fn(budget_x, budget_y)\n",
    "\n",
    "    budget_line = go.Scatter3d(\n",
    "        x=budget_x.tolist(),\n",
    "        y=budget_y.tolist(),\n",
    "        z=budget_z.tolist(),\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"green\", width=5),\n",
    "        name=\"Budget Constraint\",\n",
    "    )\n",
    "\n",
    "    # Create minimum constraints\n",
    "    min_δ_x = np.ones(100) * 10000\n",
    "    min_δ_y = np.linspace(0.01, 100000, 100)\n",
    "    min_δ_z = profit_fn(min_δ_x, min_δ_y)\n",
    "\n",
    "    min_δ_line = go.Scatter3d(\n",
    "        x=min_δ_x.tolist(),\n",
    "        y=min_δ_y.tolist(),\n",
    "        z=min_δ_z.tolist(),\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", width=5),\n",
    "        name=\"Min Digital Ad Spend\",\n",
    "    )\n",
    "\n",
    "    min_τ_x = np.linspace(0.01, 100000, 100)\n",
    "    min_τ_y = np.ones(100) * 20000\n",
    "    min_τ_z = profit_fn(min_τ_x, min_τ_y)\n",
    "\n",
    "    min_τ_line = go.Scatter3d(\n",
    "        x=min_τ_x.tolist(),\n",
    "        y=min_τ_y.tolist(),\n",
    "        z=min_τ_z.tolist(),\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"blue\", width=5),\n",
    "        name=\"Min TV Ad Spend\",\n",
    "    )\n",
    "\n",
    "    # Add optimal point\n",
    "    optimal_z = float(profit.evalf(subs={δ: 61820, τ: 38180}))\n",
    "    optimal_point = go.Scatter3d(\n",
    "        x=[61820],\n",
    "        y=[38180],\n",
    "        z=[optimal_z],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=6, color=\"green\"),\n",
    "        name=\"Optimal Point\",\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[surface, budget_line, min_δ_line, min_τ_line, optimal_point])\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Profit Function with Constraints\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Digital Ad Spend (δ)\",\n",
    "            yaxis_title=\"TV Ad Spend (τ)\",\n",
    "            zaxis_title=\"Profit (π)\",\n",
    "            camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)),\n",
    "        ),\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Save the figure\n",
    "    fig.write_html(\"data/profit_function_3d.html\")\n",
    "    fig.write_image(\"data/profit_function_3d.webp\", format=\"webp\", scale=5)\n",
    "\n",
    "\n",
    "profit_function_viz_3d()\n",
    "mo.image(\"data/profit_function_3d.webp\", height=500).center()\n",
    "# display_iframe(\"data/profit_function_3d.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {},
   "source": [
    "[View Interactive Plotly Graph](/articles/notebooks/data/profit_function_3d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def profit_function_viz_contour():\n",
    "    def log(x):\n",
    "        return np.log(x)\n",
    "\n",
    "    def profit(δ, τ):\n",
    "        profit_function = (\n",
    "            -δ\n",
    "            - τ\n",
    "            - (\n",
    "                -1.00712795746647 * log(δ)\n",
    "                - 0.621999261508067 * log(τ)\n",
    "                + 138.264406731209\n",
    "            )\n",
    "            * (10071.2795746647 * log(δ) + 6219.99261508067 * log(τ) + 17355.9326879056)\n",
    "            + 1812830.32343965 * log(δ)\n",
    "            + 1119598.67071452 * log(τ)\n",
    "            + 3124067.88382301\n",
    "        )\n",
    "\n",
    "        return profit_function\n",
    "\n",
    "    # Define the grid\n",
    "    δ = np.linspace(0.01, 100000, 1000)\n",
    "    τ = np.linspace(0.01, 100000, 1000)\n",
    "    X, Y = np.meshgrid(δ, τ)\n",
    "    Z = profit(X, Y)\n",
    "\n",
    "    # Define constraints\n",
    "    x_constraint = np.linspace(0, 100_000, 500)\n",
    "    y_constraint = 100_000 - x_constraint\n",
    "\n",
    "    # Plot contours of profit function\n",
    "    plt.figure(dpi=125)\n",
    "    contour = plt.contour(X, Y, Z, levels=100, cmap=\"plasma\")\n",
    "    plt.colorbar(contour)\n",
    "\n",
    "    plt.plot(\n",
    "        x_constraint,\n",
    "        y_constraint,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        label=r\"Constraint: $\\delta + \\tau = 100,000$\",\n",
    "    )\n",
    "\n",
    "    # # Mark the optimization points\n",
    "    plt.scatter(\n",
    "        61819.54,\n",
    "        38180.46,\n",
    "        color=\"green\",\n",
    "        marker=\"o\",\n",
    "        s=100,\n",
    "        label=\"Constrained Optimum\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Plot constraint lines\n",
    "    plt.axvline(\n",
    "        10_000,\n",
    "        color=\"black\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=1,\n",
    "        label=r\"Constraint: $\\delta \\geq 10,000$\",\n",
    "    )\n",
    "    plt.axhline(\n",
    "        20_000,\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        label=r\"Constraint: $\\tau \\geq 20,000$\",\n",
    "    )\n",
    "\n",
    "    # Shade infeasible regions in gray\n",
    "    plt.fill_betweenx(τ, 0, 10_000, color=\"gray\", alpha=0.5)  # Shade region where x > 0\n",
    "    plt.fill_between(δ, 0, 20_000, color=\"gray\", alpha=0.5)  # Shade region where y < 3\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel(r\"$\\delta$\")\n",
    "    plt.ylabel(r\"$\\tau$\")\n",
    "    plt.title(\"Contour Representation\")\n",
    "    plt.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.4))\n",
    "    plt.savefig(\n",
    "        \"data/profit_function_viz_contour.webp\",\n",
    "        format=\"webp\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "\n",
    "profit_function_viz_contour()\n",
    "mo.image(\"data/profit_function_viz_contour.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "Let’s now solve our optimization problem as formulated in eq. 12 via python using the optimization theory that we have learned from part 1 and part 2 of this series. _Note that the extremely high value of $\\rho$ is to account for the fact that the values of our objective function are extremely large thus we need to make sure penalization is large enough to avoid “jumping” out of constraints - we could normalize values for more stability._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_max():\n",
    "    objective = (\n",
    "        -profit + λ * (τ + δ - 100_000) - ρ * sm.log((τ - 20_000) * (δ - 10_000))\n",
    "    )\n",
    "\n",
    "    symbols = [δ, τ, λ, ρ]\n",
    "    x0 = {δ: 20_000, τ: 80_000, λ: 0, ρ: 100_000}\n",
    "\n",
    "    return constrained_newtons_method(objective, symbols, x0, rho_steps=1000)\n",
    "\n",
    "\n",
    "optimums = profit_max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {},
   "source": [
    "Thus, our solution is to spend ~61,800 on digital ad spend and ~38,200 on television ad spend. These values correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "digital_ad = optimums[δ]\n",
    "television_ad = optimums[τ]\n",
    "\n",
    "quantity_val = quantity_demanded.evalf(subs={δ: digital_ad, τ: television_ad})\n",
    "revenue_val = Revenue.evalf(subs={δ: digital_ad, τ: television_ad})\n",
    "cost_val = Cost.evalf(subs={δ: digital_ad, τ: television_ad})\n",
    "profit_val = revenue_val - cost_val\n",
    "\n",
    "print(f\"Quantity: {int(quantity_val):,}\")\n",
    "print(f\"Total Revenue: ${round(revenue_val,2):,}\")\n",
    "print(f\"Total Cost: ${round(cost_val,2):,}\")\n",
    "print(f\"Profit: ${round(profit_val,2):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The profit maximization problem in this final article was by no means meant to be an entirely comprehensive solution. In fact, we did not need to even use Newton’s Method for such a simple optimization problem! But, as optimization problems increase in complexity and dimensionality, which is quite common in the real world, these tools become increasingly relevant. The goal was to take what we have learned in part 1 and part 2 of this series and take a fun exploration into one of infinitely many applications of optimization theory.\n",
    "\n",
    "If you have made it up to this point, thank you for taking time out of your day to read my article and I want to extend an extra thank you to those individuals that have read through all 3 parts of this series. I hope at this point you feel very comfortable with basic multi-dimensional optimization theory and extensions involving constraints on the objective function. As always, I hope you have enjoyed reading this as much as I enjoyed writing it. Please let me know what you thought of this article and the series as whole!\n",
    "\n",
    "## Bonus - Numerical and Analytical Solutions to Linear Regression\n",
    "\n",
    "As promised above, this section will provide code for solving the linear regression problem above utilizing Newton's method and we will compare this result to the analytical closed-form solution & statsmodels directly. This exercise will provide a very elegant connection between model fitting, optimization, & different techniques for doing this! Recall that the objective for solving the linear regression is to minimize the [Residual Sum of Squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares). That is, in terms of matrices,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{\\beta}  (y-\\mathbf{X}\\beta)^T(y-\\mathbf{X}\\beta)\n",
    "\\tag{A1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, using our Newton method function and framework, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_newtons_method(input_df: pd.DataFrame):\n",
    "    # Pull all variables in X and create them as SymPy symbols\n",
    "    variablez = list(input_df.drop([\"quantity_demanded\", \"July\"], axis=1).columns)\n",
    "    symbols = []\n",
    "    for i in variablez:\n",
    "        i = sm.symbols(f\"{i}\")\n",
    "        symbols.append(i)\n",
    "\n",
    "    # Create vectors and matrices of outcome (y), covariates (X), and parameters(β)\n",
    "    y = np.array(input_df[\"quantity_demanded\"])\n",
    "    X = np.array(input_df.drop([\"quantity_demanded\", \"July\"], axis=1))\n",
    "    β = np.array(symbols)\n",
    "\n",
    "    # Specify objective function and starting values\n",
    "    objective = (y - X @ β).T @ (y - X @ β)  # Residual Sum of Squares\n",
    "    β_0 = dict(zip(symbols, [0] * len(symbols)))\n",
    "\n",
    "    return newtons_method(objective, symbols, β_0)\n",
    "\n",
    "\n",
    "β_numerical = ols_newtons_method(df_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kqZH",
   "metadata": {},
   "source": [
    "Next we will compute the analytical solution. That is, if we take the derivative of eq. A1 and set it equal to zero and solve for $\\beta$, we obtain:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\beta^* = (X^TX)^{-1}X^Ty\n",
    "\\tag{A2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Coding this, we have (we also provide analytical standard errors to compare to statsmodels, see the [OLS wiki page](https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation) if you are interested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_analytical(input_df: pd.DataFrame):\n",
    "    y = np.array(input_df[\"quantity_demanded\"])\n",
    "    X = np.array(input_df.drop([\"quantity_demanded\", \"July\"], axis=1))\n",
    "\n",
    "    # OLS Analytical Solution\n",
    "    β_analytical = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    # Compute standard errors\n",
    "    df_residuals = len(X) - len(β_analytical)\n",
    "    σ2 = 1 / df_residuals * ((y - X @ β_analytical).T @ (y - X @ β_analytical))  # MSE\n",
    "    Σ = σ2 * np.linalg.inv(X.T @ X)\n",
    "    standard_errors = np.sqrt(np.diag(Σ))\n",
    "\n",
    "    return β_analytical, standard_errors\n",
    "\n",
    "\n",
    "β_analytical, standard_errors = ols_analytical(df_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {},
   "source": [
    "Now, comparing all these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "ols_results = pd.DataFrame()\n",
    "\n",
    "ols_results[\"variable\"] = list(\n",
    "    df_mod.drop([\"quantity_demanded\", \"July\"], axis=1).columns\n",
    ")\n",
    "ols_results[\"β_numerical\"] = list(β_numerical.values())\n",
    "ols_results[\"β_analytical\"] = β_analytical\n",
    "ols_results[\"std_err_analytical\"] = standard_errors\n",
    "ols_results[\"β_statsmodels\"] = list(results.params)  # from statsmodels code above\n",
    "ols_results[\"std_err_statsmodels\"] = list(results.bse)  # from statsmodels code above\n",
    "ols_results = ols_results.set_index(\"variable\")\n",
    "\n",
    "ols_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SdmI",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 24px;\">❖❖❖</div>\n",
    "\n",
    "<center>\n",
    "Access all the code via this Marimo Notebook or my [GitHub Repo](https://github.com/jakepenzak/blog-posts)\n",
    "\n",
    "I appreciate you reading my post! My posts primarily explore real-world and theoretical applications of econometric and statistical/machine learning techniques, but also whatever I am currently interested in or learning 😁. At the end of the day, I write to learn! I hope to make complex topics slightly more accessible to all.\n",
    "</center>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
