{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Relevant Imports\n",
    "\n",
    "import marimo as mo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.chdir(\"assets/articles/notebooks\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "np.random.seed(00)\n",
    "\n",
    "## Helper Plots\n",
    "\n",
    "COLORS = [\"#00B0F0\", \"#FF0000\", \"#B0F000\"]\n",
    "\n",
    "\n",
    "def plot_effect(effect_true, effect_pred, save_path, figsize=(8, 5), ylim=(-10, 100)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(effect_true, effect_pred, color=COLORS[0], s=10)\n",
    "    plt.plot(\n",
    "        np.sort(effect_true),\n",
    "        np.sort(effect_true),\n",
    "        color=COLORS[1],\n",
    "        alpha=0.7,\n",
    "        label=\"Perfect model\",\n",
    "    )\n",
    "    plt.xlabel(\"True effect\", fontsize=14)\n",
    "    plt.ylabel(\"Predicted effect\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path, format=\"webp\", dpi=200)\n",
    "\n",
    "\n",
    "def hist_effect(effect_true, effect_pred, save_path, figsize=(8, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.hist(\n",
    "        effect_pred,\n",
    "        color=\"r\",\n",
    "        alpha=0.8,\n",
    "        density=True,\n",
    "        bins=50,\n",
    "        label=\"Linear DML CATE Prediction\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        effect_true,\n",
    "        color=\"b\",\n",
    "        alpha=0.4,\n",
    "        density=True,\n",
    "        bins=50,\n",
    "        label=\"True CATE\",\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path, format=\"webp\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Double Machine Learning, Simplified: Part 2 - Targeting & the CATE\n",
    "<center> **Learn how to utilize DML for estimating idiosyncratic treatment effects to enable personalized targeting** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> This article is the **2nd** in a 2 part series on simplifying and democratizing Double Machine Learning. In the <a href=\"/articles/dml1\" target=\"_blank\" rel=\"noopener noreferrer\">1st part</a>, we covered the fundamentals of Double Machine Learning, along with two basic causal inference applications. Now, in pt. 2, we will extend this knowledge to turn our causal inference problem into a prediction task, wherein we predict individual level treatment effects to aid in decision making and data-driven targeting\n",
    "\n",
    "Double Machine Learning, as we learned in [part 1](/articles/dml1) of this series, is a highly flexible partially-linear causal inference method for estimating the average treatment effect (ATE) of a treatment. Specifically, it can be utilized to model highly non-linear confounding relationships in observational data (especially when our set of controls/confounders is of extremely high dimensionality) and/or to reduce the variation in our key outcome in experimental settings. Estimating the ATE is particularly useful in understanding the average impact of a specific treatment, which can be extremely useful for future decision making. However, extrapolating this treatment effect assumes a degree homogeneity in the effect; that is, regardless of the population we roll treatment out to, we anticipate the effect to be similar to the ATE. What if we are limited in the number of individuals who we can target for future rollout and thus want to understand among which subpopulations the treatment was most effective to drive highly effective rollout?\n",
    "\n",
    "This issue described above concerns estimating treatment effect heterogeneity. That is, how does our treatment effect impact different subsets of the population? Luckily for us, DML provides a powerful framework to do exactly this. Specifically, we can make use of DML to estimate the Conditional Average Treatment Effect (CATE). First, let’s revisit our definition of the ATE, in binary and continuous cases, respectively:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{ATE}=\\mathbb{E_n}[y(T=1)-y(T=0)]\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{ATE}=\\mathbb{E_n}\\left[\\frac{\\partial y}{\\partial T}\\right]\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now with the CATE, we estimate the ATE conditional on a set of values for our covariates, $\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{CATE}=\\mathbb{E_n}[y(T=1)-y(T=0)|\\mathbf{X}=x]\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{CATE}=\\mathbb{E_n}\\left[\\frac{\\partial y}{\\partial T}\\right|\\mathbf{X}=x]\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For example, if we wanted to know the treatment effect for males versus females, we can estimate the CATE conditional on the covariate being equal to each subgroup of interest. Note that we can estimate highly aggregated CATEs (i.e., at a male vs. female level), also known as Group Average Treatment Effects (GATEs), or we can allow $\\mathbf{X}$ to take on an extremely high dimensionality and thus closely estimate each individuals treatment effect. You may immediately notice the benefits in being able to do this: we can utilize this information to make highly informed decisions in future targeting of the treatment! Even more notable, we can create a CATE function to make predictions of the treatment effect on previously unexposed individuals!\n",
    "\n",
    "Note, that there are many models that exist for estimating CATEs, which we'll cover in a subsequent post. For now, we'll cover two techniques within the partially linear DML formulation for estimating this CATE function; namely, Linear DML and Non-Parametric DML. Er will show how to estimate the CATE mathematically and then provide examples for each case.\n",
    "\n",
    "> Note: Unbiased estimation of the CATE still requires the exogeneity/CIA/Ignorability assumption to hold as covered in part 1.\n",
    "\n",
    "**Everything demonstrated below can and should be extended to the experimental setting (RCT or A/B Testing), where exogeneity is satisfied by construction, as covered in application 2 of part 1.**\n",
    "\n",
    "## Linear DML for Estimating the CATE\n",
    "\n",
    "Estimating the CATE in the linear DML framework is a simple extension of DML for estimating the ATE:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y-\\mathcal{M}_y(\\mathbf{X})=\\beta_0+\\beta_1(T-\\mathcal{M}_T(\\mathbf{X}))+\\epsilon\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $y$ is our outcome, $T$ is our treatment, & $\\mathcal{M}_y$ and $\\mathcal{M}_T$ are both flexible ML models (our nuisance functions) to predict $y$ and $T$ given confounders and/or controls, $\\mathbf{X}$, respectively. To estimate the CATE function using Linear DML, we can simply include interaction terms of the treatment residuals with our covariates. Observe:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y-\\mathcal{M}_y(\\mathbf{X})=\\beta_0+\\beta_1(T-\\mathcal{M}_T(\\mathbf{X}))+(T-\\mathcal{M}_T(\\mathbf{X}))\\mathbf{X}\\mathbf{\\Omega} + \\epsilon\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Omega}$ is the vector of coefficients for the interaction terms. Now our CATE function, call it $\\tau$, takes the form $\\tau(\\mathbf{X}) = \\beta_1 + \\mathbf{X}\\mathbf{\\Omega}$, where we can predict each individuals CATE given $\\mathbf{X}$. If $T$ is continuous, this CATE function is for a 1 unit increase in T. Note that $\\tau(\\mathbf{X}) = \\beta_1$ in eq. (3) where $\\tau(\\mathbf{X})$ is assumed a constant. Let’s take a look at this in action!\n",
    "\n",
    "First, let’s use the same casual DAG from part 1, where we will be looking at the effect of an individuals time spent on the website on their purchase amount, or sales, in the past month (assuming we observe all confounders).:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_dag():\n",
    "    # Create a directed graph\n",
    "    g = graphviz.Digraph(format=\"png\")\n",
    "\n",
    "    # Add nodes\n",
    "    nodes = [\n",
    "        \"Age\",\n",
    "        \"# Social Media Accounts\",\n",
    "        \"Yrs Member\",\n",
    "        \"Time on Website\",\n",
    "        \"Sales\",\n",
    "        \"Z\",\n",
    "    ]\n",
    "    [g.node(n) for n in nodes]\n",
    "\n",
    "    g.edge(\"Age\", \"Time on Website\")\n",
    "    g.edge(\"# Social Media Accounts\", \"Time on Website\")\n",
    "    g.edge(\"Yrs Member\", \"Time on Website\")\n",
    "    g.edge(\"Age\", \"Sales\")\n",
    "    g.edge(\"# Social Media Accounts\", \"Sales\")\n",
    "    g.edge(\"Yrs Member\", \"Sales\")\n",
    "    g.edge(\"Time on Website\", \"Sales\", color=\"red\")\n",
    "    g.edge(\"Z\", \"Sales\")\n",
    "\n",
    "    g.graph_attr[\"dpi\"] = \"400\"\n",
    "\n",
    "    # Render for print\n",
    "    g.render(\"data/dag1\", format=\"webp\")\n",
    "\n",
    "\n",
    "create_dag()\n",
    "mo.image(\"data/dag1.webp\").center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "Let’s then simulate this DGP using a similar process as utilized in part 1 (note that all values & data are chosen and generated arbitrarily for demonstrative purposes). Observe that we now include interaction terms in the sales DGP to model the CATE, or treatment effect heterogeneity (note that the DGP in part 1 had no treatment effect heterogeneity by construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Size\n",
    "N = 100_000\n",
    "\n",
    "# Confounders (X)\n",
    "age = np.random.randint(low=18, high=75, size=N)\n",
    "num_social_media_profiles = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=N)\n",
    "yr_membership = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=N)\n",
    "\n",
    "# Arbitrary Covariates (Z)\n",
    "Z = np.random.normal(loc=50, scale=25, size=N)\n",
    "\n",
    "# Error Terms\n",
    "ε1 = np.random.normal(loc=20, scale=5, size=N)\n",
    "ε2 = np.random.normal(loc=40, scale=15, size=N)\n",
    "\n",
    "\n",
    "# Treatment (T = g(X) + ε1)\n",
    "def T(age, num_social_media_profiles, yr_membership, ε1):\n",
    "    time_on_website = np.maximum(\n",
    "        10\n",
    "        - 0.01 * age\n",
    "        - 0.001 * age**2\n",
    "        + num_social_media_profiles\n",
    "        - 0.01 * num_social_media_profiles**2\n",
    "        - 0.01 * (age * num_social_media_profiles)\n",
    "        + 0.2 * yr_membership\n",
    "        + 0.001 * yr_membership**2\n",
    "        - 0.01 * (age * yr_membership)\n",
    "        + 0.2 * (num_social_media_profiles * yr_membership)\n",
    "        + 0.01\n",
    "        * (num_social_media_profiles * np.log(age) * age * yr_membership ** (1 / 2))\n",
    "        + ε1,\n",
    "        0,\n",
    "    )\n",
    "    return time_on_website\n",
    "\n",
    "\n",
    "time_on_website = T(age, num_social_media_profiles, yr_membership, ε1)\n",
    "\n",
    "\n",
    "# Outcome (y = f(T,X,Z) + ε2)\n",
    "def y(time_on_website, age, num_social_media_profiles, yr_membership, Z, ε2):\n",
    "    sales = np.maximum(\n",
    "        25\n",
    "        + 5 * time_on_website  # Baseline Treatment Effect\n",
    "        - 0.2 * time_on_website * age  # Heterogeneity\n",
    "        + 2 * time_on_website * num_social_media_profiles  # Heterogeneity\n",
    "        + 2 * time_on_website * yr_membership  # Heterogeneity\n",
    "        - 0.1 * age\n",
    "        - 0.001 * age**2\n",
    "        + 8 * num_social_media_profiles\n",
    "        - 0.1 * num_social_media_profiles**2\n",
    "        - 0.01 * (age * num_social_media_profiles)\n",
    "        + 2 * yr_membership\n",
    "        + 0.1 * yr_membership**2\n",
    "        - 0.01 * (age * yr_membership)\n",
    "        + 3 * (num_social_media_profiles * yr_membership)\n",
    "        + 0.1\n",
    "        * (num_social_media_profiles * np.log(age) * age * yr_membership ** (1 / 2))\n",
    "        + 0.5 * Z\n",
    "        + ε2,\n",
    "        0,\n",
    "    )\n",
    "    return sales\n",
    "\n",
    "\n",
    "sales = y(time_on_website, age, num_social_media_profiles, yr_membership, Z, ε2)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.array(\n",
    "        [sales, time_on_website, age, num_social_media_profiles, yr_membership, Z]\n",
    "    ).T,\n",
    "    columns=[\n",
    "        \"sales\",\n",
    "        \"time_on_website\",\n",
    "        \"age\",\n",
    "        \"num_social_media_profiles\",\n",
    "        \"yr_membership\",\n",
    "        \"Z\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "Now, to estimate our CATE function, as outlined in eq. (4), we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DML Procedure for Estimating the CATE\n",
    "M_sales = GradientBoostingRegressor()\n",
    "M_time_on_website = GradientBoostingRegressor()\n",
    "\n",
    "df[\"residualized_sales\"] = df[\"sales\"] - cross_val_predict(\n",
    "    M_sales,\n",
    "    df[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]],\n",
    "    df[\"sales\"],\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "df[\"residualized_time_on_website\"] = df[\"time_on_website\"] - cross_val_predict(\n",
    "    M_time_on_website,\n",
    "    df[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]],\n",
    "    df[\"time_on_website\"],\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "DML_model = smf.ols(\n",
    "    formula=\"residualized_sales ~ 1 + residualized_time_on_website + residualized_time_on_website:age + residualized_time_on_website:num_social_media_profiles + residualized_time_on_website:yr_membership\",\n",
    "    data=df,\n",
    ").fit()\n",
    "\n",
    "print(DML_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "Here we can see that linear DML closely modeled the true DGP for the CATE (see coefficients on interaction terms in sales DGP). Let’s evaluate the performance of our CATE function by comparing the linear DML predictions to the true CATE for a 1 hour increase in time on the spent on the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict CATE of 1 hour increase\n",
    "linear_dml_cates = DML_model.predict(\n",
    "    df.assign(residualized_time_on_website=lambda x: x.residualized_time_on_website + 1)\n",
    ") - DML_model.predict(df)\n",
    "\n",
    "# True CATE of 1 hour increase\n",
    "X = [age, num_social_media_profiles, yr_membership, Z, ε2]\n",
    "true_cates = y(time_on_website + 1, *X) - y(time_on_website, *X)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(true_cates, linear_dml_cates)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(true_cates, linear_dml_cates)}\")\n",
    "print(f\"R-Squared: {r2_score(true_cates, linear_dml_cates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "Plotting the distributions of the predicted CATE and true CATE, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "hist_effect(true_cates, linear_dml_cates, save_path=\"data/linear_dml_hist.webp\")\n",
    "\n",
    "mo.image(\"data/linear_dml_hist.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {},
   "source": [
    "Additionally, plotting the predicted values versus the true values we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_effect(true_cates, linear_dml_cates, save_path=\"data/linear_dml_line.webp\")\n",
    "\n",
    "mo.image(\"data/linear_dml_line.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "Overall, we have pretty impressive performance! However, the primary limitation in this approach is that we must manually specify the functional form of the CATE function, thus if we are only including linear interaction terms we may not capture the true CATE function. In our example, we simulated the DGP to only have these linear interaction terms and thus the performance is strong by construction, but let’s see what happens when we tweak the DGP for the CATE to be arbitrarily non-linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome (y = f(T,X,Z) + ε2)\n",
    "def y_fn_nonlinear(\n",
    "    time_on_website, age, num_social_media_profiles, yr_membership, Z, ε2\n",
    "):\n",
    "    sales = np.maximum(\n",
    "        25\n",
    "        + 5 * time_on_website  # Baseline Treatment Effect\n",
    "        - 0.2 * time_on_website * age  # Heterogeneity\n",
    "        - 0.0005 * time_on_website * age**2  # Heterogeneity\n",
    "        + 0.8 * time_on_website * num_social_media_profiles  # Heterogeneity\n",
    "        + 0.001 * time_on_website * num_social_media_profiles**2  # Heterogeneity\n",
    "        + 0.8 * time_on_website * yr_membership  # Heterogeneity\n",
    "        + 0.001 * time_on_website * yr_membership**2  # Heterogeneity\n",
    "        + 0.005\n",
    "        * time_on_website\n",
    "        * yr_membership\n",
    "        * num_social_media_profiles\n",
    "        * age  # Heterogeneity\n",
    "        + 0.005\n",
    "        * time_on_website\n",
    "        * (yr_membership**3 / (1 + num_social_media_profiles**2))\n",
    "        * np.log(age) ** 2\n",
    "        - 0.1 * age\n",
    "        - 0.001 * age**2\n",
    "        + 8 * num_social_media_profiles\n",
    "        - 0.1 * num_social_media_profiles**2\n",
    "        - 0.01 * (age * num_social_media_profiles)\n",
    "        + 2 * yr_membership\n",
    "        + 0.1 * yr_membership**2\n",
    "        - 0.01 * (age * yr_membership)\n",
    "        + 3 * (num_social_media_profiles * yr_membership)\n",
    "        + 0.1\n",
    "        * (num_social_media_profiles * np.log(age) * age * yr_membership ** (1 / 2))\n",
    "        + 0.5 * Z\n",
    "        + ε2,\n",
    "        0,\n",
    "    )\n",
    "    return sales\n",
    "\n",
    "\n",
    "sales_nonlinear = y_fn_nonlinear(\n",
    "    time_on_website, age, num_social_media_profiles, yr_membership, Z, ε2\n",
    ")\n",
    "\n",
    "df_nonlinear = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            sales_nonlinear,\n",
    "            time_on_website,\n",
    "            age,\n",
    "            num_social_media_profiles,\n",
    "            yr_membership,\n",
    "            Z,\n",
    "        ]\n",
    "    ).T,\n",
    "    columns=[\n",
    "        \"sales\",\n",
    "        \"time_on_website\",\n",
    "        \"age\",\n",
    "        \"num_social_media_profiles\",\n",
    "        \"yr_membership\",\n",
    "        \"Z\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "Fitting our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DML Procedure\n",
    "M_sales2 = GradientBoostingRegressor()\n",
    "M_time_on_website2 = GradientBoostingRegressor()\n",
    "\n",
    "df_nonlinear[\"residualized_sales\"] = df_nonlinear[\"sales\"] - cross_val_predict(\n",
    "    M_sales2,\n",
    "    df_nonlinear[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]],\n",
    "    df_nonlinear[\"sales\"],\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "df_nonlinear[\"residualized_time_on_website\"] = df_nonlinear[\n",
    "    \"time_on_website\"\n",
    "] - cross_val_predict(\n",
    "    M_time_on_website2,\n",
    "    df_nonlinear[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]],\n",
    "    df_nonlinear[\"time_on_website\"],\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "DML_model_nonlinear = smf.ols(\n",
    "    formula=\"residualized_sales ~ 1 + residualized_time_on_website + residualized_time_on_website:age + residualized_time_on_website:num_social_media_profiles + residualized_time_on_website:yr_membership\",\n",
    "    data=df_nonlinear,\n",
    ").fit()\n",
    "\n",
    "print(DML_model_nonlinear.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "And then evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict CATE of 1 hour increase\n",
    "linear_dml_cates_nonlinear = DML_model_nonlinear.predict(\n",
    "    df_nonlinear.assign(\n",
    "        residualized_time_on_website=lambda x: x.residualized_time_on_website + 1\n",
    "    )\n",
    ") - DML_model_nonlinear.predict(df_nonlinear)\n",
    "\n",
    "# True CATE of 1 hour increase\n",
    "true_cates_nonlinear = y_fn_nonlinear(time_on_website + 1, *X) - y_fn_nonlinear(\n",
    "    time_on_website, *X\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Mean Squared Error: {mean_squared_error(true_cates_nonlinear, linear_dml_cates_nonlinear)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean Absolute Error: {mean_absolute_error(true_cates_nonlinear, linear_dml_cates_nonlinear)}\"\n",
    ")\n",
    "print(f\"R-Squared: {r2_score(true_cates_nonlinear, linear_dml_cates_nonlinear)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "hist_effect(\n",
    "    true_cates_nonlinear,\n",
    "    linear_dml_cates_nonlinear,\n",
    "    save_path=\"data/linear_dml_nonlinear_hist.webp\",\n",
    ")\n",
    "\n",
    "mo.image(\"data/linear_dml_nonlinear_hist.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_effect(\n",
    "    true_cates_nonlinear,\n",
    "    linear_dml_cates_nonlinear,\n",
    "    save_path=\"data/linear_dml_nonlinear_line.webp\",\n",
    ")\n",
    "\n",
    "mo.image(\"data/linear_dml_nonlinear_line.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvdt",
   "metadata": {},
   "source": [
    "Here we see much degradation in performance. This non-linearity in the CATE function is precisely where Non-Parametric DML can shine!\n",
    "\n",
    "## Non-Parametric DML for Estimating the CATE\n",
    "\n",
    "Non-Parametric DML goes one step further and allows for another flexible non-parametric ML model to be utilized for learning the CATE function! Let’s take a look at how we can, mathematically, do exactly this. Let $\\tau(\\mathbf{X})$ continue to denote our CATE function. Let’s start with defining our error term relative to eq. 3 (note we drop the intercept $\\beta_0$ as this parameter is partialled out in residualization step; we could similarly drop this in the linear DML formulation, but for the sake of simplicity and consistency with part 1, we do not do this):\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y-\\mathcal{M}_y(\\mathbf{X})&=\\tau(\\mathbf{X})(T-\\mathcal{M}_T(\\mathbf{X}))+\\epsilon \\\\\n",
    "\\tilde{y} &=\\tau(\\mathbf{X})\\tilde{T}+\\epsilon \\\\\n",
    "\\epsilon&=\\tilde{y}-\\tau(\\mathbf{X})\\tilde{T}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then define the causal loss function as such (note this is just the MSE!):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathscr{L}(\\tau(\\mathbf{X}))\n",
    "&= \\frac{1}{N}\\sum_{i=1}^N\\bigl(\\tilde{y}_i - \\tau(\\mathbf{X}_i)\\tilde{T}_i\\bigr)^2 \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^N\\bigl(\\tilde{y}_i - \\tau(\\mathbf{X}_i)\\tilde{T}_i\\bigr)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "What does this mean? We can directly learn $\\tau(\\mathbf{X})$ with any flexible ML model via minimizing our causal loss function! This amounts to a weighted regression problem with our target and weights, respectively, as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Target}&=\\frac{\\tilde{y}_i}{\\tilde{T}_i} \\\\\n",
    "\\text{Weights}&=\\tilde{T}_i^2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "_Take a moment and soak in the elegance of this result… We can directly learn the CATE function & predict an individuals CATE given our residualized outcome, $y$, and treatment, $T$!_\n",
    "\n",
    "Let’s take a look at this in action now. We will reuse the DGP for the non-linear CATE function that was utilized in the example where linear DML performs poorly above. To construct of Non-Parametric DML model, we can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {},
   "source": [
    "Then define the causal loss function as such (note this is just the MSE!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target & Weights\n",
    "df_nonlinear[\"target\"] = (\n",
    "    df_nonlinear[\"residualized_sales\"] / df_nonlinear[\"residualized_time_on_website\"]\n",
    ")\n",
    "df_nonlinear[\"weights\"] = df_nonlinear[\"residualized_time_on_website\"] ** 2\n",
    "\n",
    "# Non-Parametric CATE Model\n",
    "CATE_model = GradientBoostingRegressor()\n",
    "CATE_model.fit(\n",
    "    df_nonlinear[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]],\n",
    "    df_nonlinear[\"target\"],\n",
    "    sample_weight=df_nonlinear[\"weights\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "And to make predictions + evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict CATE of 1 hour increase\n",
    "nonparam_dml_cates_nonlinear = CATE_model.predict(\n",
    "    df_nonlinear[[\"age\", \"num_social_media_profiles\", \"yr_membership\"]]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Mean Squared Error: {mean_squared_error(true_cates_nonlinear, nonparam_dml_cates_nonlinear)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean Absolute Error: {mean_absolute_error(true_cates_nonlinear, nonparam_dml_cates_nonlinear)}\"\n",
    ")\n",
    "print(f\"R-Squared: {r2_score(true_cates_nonlinear, nonparam_dml_cates_nonlinear)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "hist_effect(\n",
    "    true_cates_nonlinear,\n",
    "    nonparam_dml_cates_nonlinear,\n",
    "    save_path=\"data/nonparam_dml_nonlinear_hist.webp\",\n",
    ")\n",
    "\n",
    "mo.image(\"data/nonparam_dml_nonlinear_hist.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_effect(\n",
    "    true_cates_nonlinear,\n",
    "    nonparam_dml_cates_nonlinear,\n",
    "    save_path=\"data/nonparam_dml_nonlinear_line.webp\",\n",
    ")\n",
    "\n",
    "mo.image(\"data/nonparam_dml_nonlinear_line.webp\", height=500).center()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {},
   "source": [
    "Here we can see that, although not perfect, the non-parametric DML approach was able to model the non-linearities in the CATE function much better than the linear DML approach. We can of course further improve the performance via tuning our model. Note that we can use explainable AI tools, such as [SHAP values](https://shap.readthedocs.io/en/latest/index.html), to understand the nature of our treatment effect heterogeneity!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it! Thank you for taking the time to read through my article. I hope this article has taught you how to go beyond estimating only the ATE & utilize DML to estimate the CATE to further understanding heterogeneity in the treatment effects and drive more causal inference- & data- driven targeting schemes.\n",
    "\n",
    "As always, I hope you have enjoyed reading this as much as I enjoyed writing it!\n",
    "\n",
    "## References\n",
    "[1] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a. W. Newey. Double Machine Learning for Treatment and Causal Parameters. ArXiv e-prints, July 2016.\n",
    "\n",
    "<div style=\"text-align: center; font-size: 24px;\">❖❖❖</div>\n",
    "\n",
    "<center>\n",
    "Access all the code via this Marimo Notebook or my [GitHub Repo](https://github.com/jakepenzak/blog-posts)\n",
    "\n",
    "I appreciate you reading my post! My posts primarily explore real-world and theoretical applications of econometric and statistical/machine learning techniques, but also whatever I am currently interested in or learning 😁. At the end of the day, I write to learn! I hope to make complex topics slightly more accessible to all.\n",
    "</center>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
